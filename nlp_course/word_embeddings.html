<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>词嵌入</title>
    <meta name="description" content="">

    <link rel="stylesheet" href="../css/main.css">
    <link rel="canonical" href="http://lena-voita.github.io/nlp_course/word_embeddings.html">

    <!-- diff from head.html begin -->
    <link rel="mask-icon" href="../resources/lectures/ico/course_logo.png">
    <link rel="alternate icon" class="js-site-favicon" type="image/png"
        href="../resources/lectures/ico/course_logo.png">
    <link rel="icon" class="js-site-favicon" type="image/svg+xml" href="../resources/lectures/ico/course_logo.png">
    <!-- diff from head.html end -->

</head>


<body>

    <header class="site-header">

        <div class="wrapper">

            <div id="title-image" style="display:inline">
                <img class="site-img" src="../img/ico/logo.jpeg" />
            </div>

            <div id="title-texts" style="display:inline">
                <a class="site-title" href="https://github.com/MLNLP-World" target="_blank">MLNLP</a>
            </div>

            <nav class="site-nav">
                <a href="#" class="menu-icon">
                    <svg viewBox="0 0 18 15">
                        <path fill="#424242"
                            d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z" />
                        <path fill="#424242"
                            d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z" />
                        <path fill="#424242"
                            d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z" />
                    </svg>
                </a>

                <div class="trigger">
                    <a class="page-link" href="https://lena-voita.github.io/" target="_blank">原作者：Lena Voita</a>
                    <a class="page-link" href="https://lena-voita.github.io/nlp_course/word_embeddings.html"
                        target="_blank">原始英文版本</a>
                    <a class="page-link" href="../about.html" target="_blank">关于我们</a>
                </div>
            </nav>

        </div>

    </header>


    <!-- the next two lines are inserted once per page even if there are several shtukovinas,
     the rest is one-per-shtukovina; read more: https://flickity.metafizzy.co/options.html -->
    <link rel="stylesheet" href="https://unpkg.com/flickity@2/dist/flickity.min.css" media="screen">
    <script src="https://unpkg.com/flickity@2/dist/flickity.pkgd.min.js"></script>

    <!-- MathJax -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <style>
        .demo_sidebar {
            margin: 0;
            padding: 0;
            background-color: white;

            position: relative;
            height: auto;
            width: 290px;
        }

        .demo_sidebar_text {
            font-size: 18px;
        }

        .demo_sidebar_comment {
            font-size: 15px;
            margin-left: 10px;
            font-family: "Comic Neue", "Arial";

        }


        .demo_sidebar a {
            display: block;
            color: black;
            padding: 8px;
            text-decoration: none;

        }

        .demo_sidebar a li {
            margin-left: 5px;
            padding: 0px;
        }


        .demo_sidebar a:hover {
            box-shadow: 0 3px 6px #6e8f27, 0 1px 1px #6e8f27;
        }

        .demo_sidebar a:hover:not(.active) {
            background-color: #f3f3f3;
        }

        .demo_sidebar .main_components {
            background-color: #f3f3f3;
        }

        .demo_sidebar .extra_components {
            background-color: #eaeaea;
        }


        #demo_sidebar_research_thinking:hover {
            box-shadow: 0 3px 6px #a68305, 0 1px 1px #a68305;

        }


        #demo_sidebar_related_papers:hover {
            box-shadow: 0 3px 6px #8a4972, 0 1px 1px #8a4972;
        }


        #demo_sidebar_fun:hover {
            box-shadow: 0 3px 6px #377b94, 0 1px 1px #377b94;
        }
    </style>


    <style>
        @import url('https://fonts.googleapis.com/css?family=Josefin+Sans&display=swap');
        @import url("https://fonts.googleapis.com/css2?family=Patrick+Hand&display=swap");
        @import url("https://fonts.googleapis.com/css2?family=Comic+Neue&display=swap");


        #main_page_content {
            margin-left: 300px;
            padding: 30px;
            padding-left: 70px;
            text-align: justify;
        }

        .sidebar {
            margin: 0;
            padding: 0;
            width: 270px;
            background-color: #fafafa;
            position: fixed;
            height: 100%;
            overflow: auto;
            z-index: 1;
        }

        .sidebar a,
        .dropdown-btn {
            display: block;
            color: black;
            padding: 8px;
            text-decoration: none;
            border-right: 5px solid #b7db67;
        }


        .sidebar a li {
            margin-left: 10px;
            padding: 0px;
        }

        .dropdown-container {
            display: none;
            background-color: #f4f4f4;
        }

        .active_caret .fa-caret-down {
            color: #b7db67;
            font-size: 30px;
        }

        .fa-caret-down {
            float: right;
            padding-right: 8px;
        }

        .sidebar a.active {
            background-color: #e3e3e3;
            color: black;
        }

        .sidebar a:hover {
            box-shadow: 0 3px 6px #6e8f27, 0 1px 1px #6e8f27;
        }

        .sidebar a:hover:not(.active) {
            background-color: #f3f3f3;
        }

        .sidebar .extra_components {
            background-color: #eaeaea;
        }

        #sidebar_analysis {
            padding: 10px;
        }

        #sidebar_research_thinking {
            padding: 10px;
            border-right: 5px solid #fad400;
        }

        #sidebar_research_thinking:hover {
            box-shadow: 0 3px 6px #a68305, 0 1px 1px #a68305;

        }


        #sidebar_related_papers {
            padding: 10px;
            border-right: 5px solid #d192ba;
        }

        #sidebar_related_papers:hover {
            box-shadow: 0 3px 6px #8a4972, 0 1px 1px #8a4972;
        }


        #sidebar_fun {
            padding: 10px;
            border-right: 5px solid #6fb7d1;
        }

        #sidebar_fun:hover {
            box-shadow: 0 3px 6px #377b94, 0 1px 1px #377b94;
        }


        .sidebar_ico {
            float: right;
            height: 20;
        }

        div.content {
            margin-left: 200px;
            padding: 1px 16px;
            height: 1000px;
        }

        #sidebar_small {
            width: 60px;
        }


        @media screen and (max-width: 1000px) {
            .sidebar {
                width: 200px;
            }

            #main_page_content {
                margin-left: 200px;
                padding-left: 50px;
            }

            #for_you_in_sidebar {
                display: none;
            }

        }


        @media screen and (max-width: 800px) {

            div.content {
                margin-left: 0;
            }

            #main_page_content {
                margin-left: 80px;
                padding: 15px;
            }

            #demo_sidebar {}
        }


        .softmax_tau_bokeh {
            font-size: 16px;
        }


        .card_with_ico {
            position: relative;
            padding: 10px;
            margin: 10px;
        }

        .card_with_ico p {
            padding: 10px;
        }

        .card_with_ico ul {
            padding: 10px;
        }

        .card_with_ico .text_box_green {
            border: 1px solid #d8e8b5;
            border-radius: 5px;
            margin-left: 30px;
        }

        .card_with_ico .text_box_pink {
            border: 1px solid #dec8d6;
            border-radius: 5px;
            margin-left: 30px;
        }

        .card_with_ico .text_box_yellow {
            border: 1px solid #f0e4a5;
            border-radius: 5px;
            margin-left: 30px;
        }

        .card_with_ico .ico {
            float: left;
            width: 25px;
        }


        .text_box_green {
            border: 1px solid #d8e8b5;
            border-radius: 5px;
            display: table;
            margin-left: 20px;
        }

        .text_box_green p {
            padding: 10px;
            padding-bottom: 0px;
        }


        .green_left_thought {
            border-left: 5px solid #b7db67;
            margin: 10px;
            margin-left: 20px;
            padding: 0px;
            background-color: #fafcf5;
        }

        .green_left_thought p {

            margin-left: 10px;
            padding: 5px;
        }


        .box_green_left {
            border-left: 2px solid #79a123;
            margin-left: 10px;
            padding: 10px;
        }

        .box_green_right {
            border-right: 2px solid #79a123;
            margin-right: 10px;
            padding: 10px;
        }

        .box_violet_right {
            border-right: 2px solid #67468f;
            margin-right: 10px;
            padding: 10px;
        }

        .box_pink_left {
            border-left: 2px solid #7a3160;
            margin-left: 5px;
            padding: 10px;
        }

        .box_yellow_left {
            border-left: 2px solid #d6b000;
            margin-left: 5px;
            padding: 10px;
        }

        .box_blue_left {
            border-left: 2px solid #0278a1;
            margin-left: 5px;
            padding: 10px;
        }

        .paper_title {
            text-align: center;
            padding: 5px;
            padding-top: 0px;
            font-size: 16px;

        }

        .paper_authors {
            font-size: 14px;
            text-align: center;
            margin-bottom: 10px;
        }

        p {
            text-align: justify;
        }


        .greenCard {
            width: 100%;
            border: 1px solid #ccc;
            border-radius: 1px;
            margin: 10px 5px;
            padding: 3px;
            display: grid;
            grid-template-rows: auto auto;
        }

        #thumbnail_green {
            box-shadow: 0 2px 4px #6e8f27, 0 1px 1px #6e8f27;
        }

        #thumbnail_green:hover {
            box-shadow: 0 6px 12px #6e8f27, 0 4px 4px #6e8f27;
        }

        #thumbnail_violet {
            box-shadow: 0 2px 4px #655578, 0 1px 1px #655578;
        }

        #thumbnail_violet:hover {
            box-shadow: 0 6px 12px #655578, 0 4px 4px #655578;
        }

        #thumbnail_paper {
            box-shadow: 0 2px 4px #ab859e, 0 1px 1px #ab859e;
        }

        #thumbnail_paper:hover {
            box-shadow: 0 6px 12px #ab859e, 0 4px 4px #ab859e;
        }


        #thumbnail_blue {
            box-shadow: 0 2px 4px #377b94, 0 1px 1px;
        }

        #thumbnail_blue:hover {
            box-shadow: 0 6px 12px #377b94, 0 4px 4px #377b94;
        }

        .paperCard {
            width: 100%;
            border: 1px solid #ccc;
            border-radius: 1px;
            margin: 10px 5px;
            padding: 3px;
            display: grid;
            grid-template-rows: auto auto;
        }

        .paperIntro {
            display: grid;
            grid-template-columns: 70% 30%;
        }

        .showMePaper {
            box-shadow: 0 2px 4px #a68305, 0 2px 1px #a68305;
        }

        .showMePaper:hover {
            box-shadow: 0 3px 6px #ab859e, 0 4px 4px #ab859e;
        }


        .cardContent {
            padding: 10px;
        }

        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

        .conf_name {
            float: right;
            font-family: sans-serif;
            font-size: 13px;
            margin-bottom: 0px;
            background-color: #f7edf4;
            border: 1px solid #d1a1c3;
            border-radius: 1px;
            padding: 0px 4px 0px 4px;
        }


        .paper_circle {
            height: 10px;
            width: 10px;
            background-color: #cf99be;
            border: 1px solid #8c2b6e;
            border-radius: 50%;
        }

        .research_circle {
            height: 10px;
            width: 10px;
            background-color: #ffda00;
            border: 1px solid #998300;
            border-radius: 50%;
        }

        .fun_circle {
            height: 10px;
            width: 10px;
            background-color: #68c7e8;
            border: 1px solid #0278a1;
            border-radius: 50%;
        }

        .green_circle {
            height: 10px;
            width: 10px;
            background-color: #b7db67;
            border: 1px solid #598005;
            border-radius: 50%;
        }

        .violet_circle {
            height: 10px;
            width: 10px;
            background-color: #b59fcf;
            border: 1px solid #67468f;
            border-radius: 50%;
        }

        .data_text {
            font-family: "Comic Neue", "Arial";
        }


        .researchCard {
            width: 100%;
            border: 1px solid #ccc;
            border-radius: 1px;
            margin: 10px 5px;
            padding: 3px;
            display: grid;
            grid-template-rows: auto auto;
        }

        .researchIntro {
            display: grid;
            grid-template-columns: 70% 30%;
        }

        #thumbnail_research {
            box-shadow: 0 2px 4px #a68305, 0 1px 1px #a68305;
        }

        #thumbnail_research:hover {
            box-shadow: 0 6px 12px #a68305, 0 4px 4px #a68305;
        }

        .research_title {
            text-align: center;
            padding: 5px;
            font-size: 18px;
            font-family: "Comic Neue", "Arial";
            background-color: #fafaf5;


        }

        .research_tag {
            float: right;
            font-family: sans-serif;
            font-size: 13px;
            margin-bottom: 0px;
            background-color: #f5f0d5;
            border: 1px solid #fad400;
            border-radius: 1px;
            padding: 0px 4px 0px 4px;
        }

        .research_question {
            font-weight: bold;
            font-size: 18px;
            background-color: #fff4b3;
            margin-right: 15px;
            padding-left: 5px;
            padding-right: 5px;
        }

        .research_summary {
            padding: 5px;
            font-size: 18px;
            font-family: "Comic Neue", "Arial";
            background-color: #fafaf5;
            margin-left: 10px;
        }


        .text_table td,
        .text_table th {
            text-align: center;
            padding-left: 10px;
            padding-right: 10px;
            padding-top: 2px;
            padding-bottom: 2px;
        }
    </style>

    <!--##################################################-->

    <div>
        <div class="sidebar" id="sidebar">
            <a href="javascript:void(0)" id="close_sidebar_btn" onclick="closeNav()"
                style="text-align:center;font-size:30px;padding:0px;">⇤</a>
            <a class="active" href="../index.html" style="font-weight: bold;">
                <img height="18" class='sidebar_ico' src="../resources/lectures/ico/course_logo.png"
                    style="margin-right: 8px;margin-left: 8px;margin-top: 4px;" />
                NLP 课程 <font color="#92bf32" id="for_you_in_sidebar">| 专属定制</font></a>
            <a href="#main_content" style="font-weight: bold;">词嵌入</a>
            <a href="#one_hot_vectors">独热向量</a>
            <a href="#distributional_semantics">分布式语义</a>
            <a href="#pre_neural">基于计数的方法</a>

            <div class="dropdown-scope">
                <a class="dropdown-btn">Word2Vec
                    <i class="fa fa-caret-down"></i>
                </a>
                <div class="dropdown-container">
                    <a href="#w2v_idea"><span style="margin-right:15px;font-size:14px;">&#8226;</span>核心思想</a>
                    <a href="#w2v_objective_function"><span
                            style="margin-right:15px;font-size:14px;">&#8226;</span>目标函数</a>
                    <a href="#w2v_training"><span style="margin-right:15px;font-size:14px;">&#8226;</span>训练过程</a>
                    <a href="#w2v_negative_sampling"><span
                            style="margin-right:15px;font-size:14px;">&#8226;</span>负采样</a>
                    <a href="#w2v_skipgram_cbow"><span style="margin-right:15px;font-size:14px;">&#8226;</span>Skip-Gram
                        和
                        CBOW</a>
                    <a href="#w2v_additional_notes"><span
                            style="margin-right:15px;font-size:14px;">&#8226;</span>附加笔记</a>
                </div>
            </div>

            <a href="#glove">GloVe</a>
            <a href="#evaluation">评价</a>
            <a href="#analysis_interpretability" id="sidebar_analysis">分析与解释 <img height="25"
                    src="../resources/lectures/ico/analysis_empty.png" class="sidebar_ico" /></a>
            <div class="extra_components">
                <a href="#research_thinking" id="sidebar_research_thinking">研究思考<img height="30"
                        src="../resources/lectures/ico/bulb_empty.png" class="sidebar_ico" /></a>
                <!--<hr color="#b7db67">-->

                <a href="#related_papers" id="sidebar_related_papers">相关论文 <img height="22"
                        src="../resources/lectures/ico/book_empty.png" class="sidebar_ico" /></a>
                <!--<hr color="#b7db67">-->

                <a href="#have_fun" id="sidebar_fun">Have Fun! <img height="30"
                        src="../resources/lectures/ico/fun_empty.png" class="sidebar_ico" /></a>
            </div>
        </div>


        <div class="sidebar" id="sidebar_small">

            <a class="active" onclick="openNav()" style="text-align:center;">☰</a>
            <a href="../nlp_course.html">
                <img height="20" src="../resources/lectures/ico/course_logo.png"
                    style="margin-right: 8px;margin-left: 8px;" /></a>
            <a href="#main_page_content" style="text-align:center; font-size:20px;color:#7ca81e"> <i
                    class="fa fa-home"></i></a>

            <a href="#analysis_interpretability" id="sidebar_analysis"> <img height="25"
                    src="../resources/lectures/ico/analysis_empty.png" /></a>
            <div class="extra_components">
                <a href="#research_thinking" id="sidebar_research_thinking"><img height="30"
                        src="../resources/lectures/ico/bulb_empty.png" /></a>
                <!--<hr color="#b7db67">-->

                <a href="#related_papers" id="sidebar_related_papers"><img height="22"
                        src="../resources/lectures/ico/book_empty.png" /></a>
                <!--<hr color="#b7db67">-->

                <a href="#have_fun" id="sidebar_fun"><img height="30"
                        src="../resources/lectures/ico/fun_empty.png" /></a>
            </div>
        </div>


        <script>
            function openNav() {
                document.getElementById("sidebar").style.display = "block";
                document.getElementById("sidebar_small").style.display = "none";
                document.getElementById("close_sidebar_btn").style.display = "block";
            }

            function closeNav() {
                document.getElementById("sidebar").style.display = "none";
                document.getElementById("sidebar_small").style.display = "block";
                document.getElementById("close_sidebar_btn").style.display = "none";
            }

        </script>


        <script>
            function onResize() {
                if (window.innerWidth >= 800) {
                    document.getElementById("sidebar").style.display = "block";
                    document.getElementById("sidebar_small").style.display = "none";
                    document.getElementById("close_sidebar_btn").style.display = "none";
                } else {
                    document.getElementById("sidebar").style.display = "none";
                    document.getElementById("sidebar_small").style.display = "block";
                }
            }

            window.onresize = onResize;
            onResize();
        </script>

        <script>
            /* Loop through all dropdown buttons to toggle between hiding and showing its dropdown content - This allows the user to have multiple dropdowns without any conflict */
            var dropdown = document.getElementsByClassName("dropdown-btn");
            var i;

            for (i = 0; i < dropdown.length; i++) {
                dropdown[i].addEventListener("click", function (event) {
                    this.classList.toggle("active_caret");
                    var dropdownButton = event.target || event.srcElement;
                    while (dropdownButton.className != "dropdown-scope")
                        dropdownButton = dropdownButton.parentElement;
                    var dropdownContent = dropdownButton.getElementsByClassName("dropdown-container")[0];

                    if (dropdownContent.style.display === "block") {
                        dropdownContent.style.display = "none";
                    } else {
                        dropdownContent.style.display = "block";
                    }
                });
            }
        </script>


        <style>
            :root {}

            .quiz_window {
                width: 100%;
                border: 1px solid #ccc;
                border-radius: 1px;
                margin: 10px 5px;
                padding: 3px;
                background-color: white;
                text-align: center;
            }

            #semantic_space_surfer {
                box-shadow: 0 2px 4px #377b94, 0 1px 1px;
            }

            #semantic_space_surfer:hover {
                box-shadow: 0 6px 12px #377b94, 0 4px 4px #377b94;
            }

            .prompt_text {
                font-size: 24px;
                font-family: "Comic Neue", "Arial";
                margin-top: 10px;
                margin-bottom: 10px;
                font-weight: bold;
                text-align: center;
                background-color: #ebf6fa;
            }

            .result_header {
                font-size: 24px;
                font-family: "Comic Neue", "Arial";
                margin-top: 10px;
                margin-bottom: 10px;
                font-weight: bold;
                text-align: center;
                background-color: #ebf6fa;
            }

            .result_course_mention {
                font-size: 18px;
                font-family: "Comic Neue", "Arial";
                margin-top: 10px;
                margin-bottom: 10px;
                text-align: center;
                background-color: white;
            }

            .comment_text {
                font-size: 16px;
                font-family: "Gill Sans", sans-serif;
                font-style: italic;
                text-align: center;
            }

            .answer_button {
                width: 40%;
                background-color: #fafafa;
                font-size: 20px;
                font-family: "Comic Neue", "Arial";
                font-weight: bold;
                color: black;
                margin-right: 20px;
                margin-left: 20px;
                margin-top: 10px;
                margin-bottom: 10px;
                border: 0px solid black;
                border-radius: 12px;
                padding: 20px;
                padding-top: 0px;
                padding-bottom: 0px;
                text-decoration: none;
                display: inline-block;
                text-align: center;
                box-shadow: 0px 2px 3px #377b94, 0 1px 1px #377b94;
            }

            .answer_button:hover {
                box-shadow: 0 3px 6px #377b94, 0 2px 2px #377b94;
            }

            .answer_text_tight {
                margin-top: 3px;
                margin-bottom: 3px;
            }

            .quiz_result {
                font-size: 24px;
                font-family: "Gill Sans", sans-serif;
                margin-top: 10px;
                margin-bottom: 10px;
            }

            .answer_block {
                display: grid;
                grid-template-columns: auto 30px;
            }

            .next_button {
                width: 0;
                height: 0;
                border-top: 50px solid transparent;
                border-left: 20px solid #1b6f8c;
                border-bottom: 50px solid transparent;
            }

            .next_text {
                color: #1b6f8c;
                font-weight: 600;
                font-family: "Comic Neue", "Arial";
                margin: 0 auto;
            }

            progress[value]::-moz-progress-bar {
                background-image: -moz-linear-gradient(135deg,
                        transparent 33%,
                        rgba(0, 0, 0, 0.1) 33%,
                        rgba(0, 0, 0, 0.1) 66%,
                        transparent 66%),
                    -moz-linear-gradient(top,
                        rgba(255, 255, 255, 0.25),
                        rgba(0, 0, 0, 0.25)),
                    -moz-linear-gradient(left,
                        #09c,
                        #44f);

                border-radius: 2px;
                background-size: 35px 20px, 100% 100%, 100% 100%;
                display: inline-block;
            }
        </style>


        <div class="wrapper" id="main_page_content">
            <div class="header">
                <h1><b>词嵌入（Word Embeddings）</b> <a
                        href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Have%20Fun!-,Word%20Embeddings,-The%20way%20machine"
                        style="font-size:9px;" target="_blank">(英文原文)</a></h1>
            </div>

            <div class="main_content" id="main_content">

                <p class="data_text">
                    <font color="black">
                        译者：<a href="http://jinjie.one/" target="_blank">倪瑾杰</a> 校对：<a
                            href="https://siviltaram.github.io/" target=" _blank">刘乾</a>
                    </font>
                </p>

                <div id="intro">

                    <img height="200" src="../resources/lectures/word_emb/word_repr_intro-min.png"
                        style="float:right; margin-left: 25px; max-width:60%" />
                    <p>机器学习模型“查看”数据的方式与我们（人类）不同。例如，我们可以很容易地理解文本“我看到了一只猫”，但模型却不能，模型需要特征向量。这样的特征向量被称为词嵌入，是一种可以输入模型的词语表示。
                    </p>

                    <br>

                    <img height="130" src="../resources/lectures/word_emb/lookup_table.gif"
                        style="float:right; margin-left: 25px; max-width:60%" />
                    <h4><u>工作原理</u>：查找表（词表） <a
                            href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=How%20it%20works%3A%20Look%2Dup%20Table%20(Vocabulary)"
                            style="font-size:9px;" target="_blank">(英文原文)</a></h4>
                    <p>
                        在实践中，你有一个事先确定好的词汇表。对于每个词汇表中的单词，查找表都有该单词对应的词嵌入，我们可以使用单词在词汇表中的索引找到该词嵌入。</p>
                    <br>


                    <img height="70" src="../resources/lectures/word_emb/unk_in_voc-min.png"
                        style="float:right; margin-left: 25px; max-width:60%" />
                    <p>为了表示未登录词（即不在词汇表中的词），词汇表通常包含一个特殊的单词
                        UNK。当然，我们也可以选择忽略未登录词，或者简单分配一个零值向量。</p>

                    <h3> 这堂课的核心问题是：我们该如何获得这些词向量？ <a
                            href="https://lena-voita.github.io/nlp_course/transfer_learning.html#:~:text=Sentence%20pairs%20classification"
                            style="font-size:9px;" target="_blank">(英文原文)</a>
                    </h3>

                </div>
                <br>

                <div id="one_hot_vectors">
                    <h2><b>离散符号表示：独热向量（One-hot Vectors）</b> <a
                            href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Represent%20as%20Discrete%20Symbols%3A%20One%2Dhot%20Vectors"
                            style="font-size:9px;" target="_blank">(英文原文)</a></h2>

                    <img height="200" src="../resources/lectures/word_emb/one_hot-min.png"
                        style="float:right; margin-left: 25px; max-width:60%" />

                    <p>最简单的词向量表示方法是用独热（One-hot）向量表示每个单词：对于词汇表中的第 i 个单词，独热向量的第 i 个维度为
                        1，其余为 0。在机器学习中，独热向量是表示分类特征最简单的方法。</p>


                    <p>
                        你大概能猜到为什么独热向量不是表示单词的最佳方式。它的一个已知问题是，独热向量在词汇表较大时会变得非常长，因为独热向量的维度就等于词汇表大小，这在实践中是不可取的。但这并不是最关键的问题。
                    </p>

                    <p>真正重要的是，独热向量对它们所代表的词语<b>一无所知</b>。例如，明显背离常识的是，独热向量居然认为
                        “猫”到“狗”和“桌子”的语义距离一样近！因此，我们认为<u>独热向量没有捕捉到<b>单词含义</b></u>。</p>

                    <p>那么问题来了，<b>我们怎么知道什么是含义呢？</b></p>

                </div>
                <br>


                <div id="distributional_semantics">
                    <h2><b>分布式语义</b> <a
                            href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=what%20is%20meaning%3F-,Distributional%20Semantics,-To%20capture%20meaning"
                            style="font-size:9px;" target="_blank">(英文原文)</a></h2>

                    <p>
                        为了捕捉单词向量表示的含义，我们首先需要定义可以在实践中使用的“含义”的概念。为此，让我们首先来了解人类如何知道哪些词具有相似的含义。
                    </p>


                    <img height="20" src="../resources/lectures/ico/paw_empty.png"
                        style="float:left; margin-top:-10px;" />
                    <div class="box_green_left">

                        <div class="text_box_green">
                            <p class="data_text"><u>How to:</u> go over the slides at your pace. Try to notice how your
                                brain works.</p>
                        </div>

                        <div class="carousel" data-flickity='{ "imagesLoaded": true, "percentPosition": true,
            "selectedAttraction": 1, "friction": 1 }'
                            style="width:100%; margin-top:10px; margin-bottom:30px; margin-left:10px;">
                            <div class="carousel-cell" style="width:100%">
                                <center>
                                    <img width="600" src="../resources/lectures/word_emb/tezguino1-min.png" />
                                </center>
                            </div>
                            <div class="carousel-cell" style="width:100%">
                                <center>
                                    <img width="600" src="../resources/lectures/word_emb/tezguino2-min.png" />
                                </center>
                            </div>
                            <div class="carousel-cell" style="width:100%">
                                <center>
                                    <img width="600" src="../resources/lectures/word_emb/tezguino3-min.png" />
                                </center>
                            </div>
                            <div class="carousel-cell" style="width:100%">
                                <center>
                                    <img width="600" src="../resources/lectures/word_emb/tezguino4-min.png" />
                                </center>
                            </div>
                            <div class="carousel-cell" style="width:100%">
                                <center>
                                    <img width="600" src="../resources/lectures/word_emb/tezguino5-min.png" />
                                </center>
                            </div>
                            <div class="carousel-cell" style="width:100%">
                                <center>
                                    <img width="600" src="../resources/lectures/word_emb/tezguino6-min.png" />
                                </center>
                            </div>
                            <div class="carousel-cell" style="width:100%">
                                <center>
                                    <img width="600" src="../resources/lectures/word_emb/tezguino7-min.png" />
                                </center>
                            </div>
                            <div class="carousel-cell" style="width:100%">
                                <center>
                                    <img width="600" src="../resources/lectures/word_emb/tezguino8-min.png" />
                                </center>
                            </div>
                            <div class="carousel-cell" style="width:100%">
                                <center>
                                    <img width="600" src="../resources/lectures/word_emb/tezguino9-min.png" />
                                </center>
                            </div>
                        </div>

                        <div style="font-size:14px; margin-left: 20px; margin-top: 20px;" class="data_text">
                            <font color="#888">
                                <u>Lena</u>: The example is from
                                <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf"
                                    target="_blank">
                                    Jacob Eisenstein's NLP notes;</a>
                                the <span style="font-weight:bold;">tezgüino</span> example
                                originally appeared in <a href="https://www.aclweb.org/anthology/C98-2122.pdf"
                                    target="_blank">Lin, 1998</a>.
                            </font>
                        </div>

                    </div>
                    <img height="20" src="../resources/lectures/ico/paw_empty.png"
                        style="float:left; margin-top:-10px;" />

                    <br><br>
                    <p>
                        人类有一种能力：一旦你看到未知词语在不同语境中的使用方式，你就能够理解它的含义。那人类是怎么做的呢？</p>

                    <p>猜想是：你的大脑搜索了其他可用于相同语境的词，找到一些单词（例如，葡萄酒），并得出结论 tezgüino
                        与这些词具有相似的含义。这就是分布假设：</p>

                    <div class="green_left_thought" style="font-size:18px;">
                        <p class="data_text">
                            经常出现在<b>相似上下文</b>中的词具有<b>相似含义</b>。<br>
                            Words which frequently appear in similar contexts have similar meaning.
                        </p>

                    </div>

                    <p class="data_text" style="color:#888; font-size:14px;">
                        <u>编者按</u>：分布假设的一个通俗版本是“观其伴而知其意” （You
                        shall know a word by the company it），源自J. R. Firth，1957。实际上，也有更早的人说过类似的话，例如
                        Harris, 1954。
                    </p>

                    <p>
                        这是一个非常有价值的想法：可以用在实践中让词向量捕捉单词的含义。根据分布假设，“捕捉含义”和“捕捉上下文”在本质上是一样的。
                        因此，我们需要做的就是将有关单词上下文的信息引入单词表示中。
                    </p>

                    <div class="green_left_thought" style="font-size:18px;">
                        <p class="data_text"><u>核心思想</u>：我们需要将有关单词上下文的信息引入单词表示中。</p>
                    </div>

                    <p>在本次课程中，我们要做的就是使用不同的方法来做到这一点。</p>
                </div>
                <br>


                <div id="pre_neural">
                    <h1><b>基于计数的方法</b> <a
                            href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=to%20do%20this.-,Count%2DBased%20Methods,-Let%27s%20remember%20our"
                            style="font-size:9px;" target="_blank">(英文原文)</a></h1>

                    <center>
                        <img src="../resources/lectures/word_emb/preneural/idea-min.png"
                            style="max-width:90%; margin-bottom: 15px;" />
                    </center>

                    <p>再来重申一下我们的核心思想：</p>
                    <div class="green_left_thought">
                        <p class="data_text" style="font-size:18px;"><u>核心思想</u>：我们需要将有关上下文的信息引入词向量中。
                        </p>
                    </div>


                    <p>基于计数的方法非常直观地实现了这个想法：</p>
                    <div class="green_left_thought">
                        <p class="data_text" style="font-size:18px;"><u>具体做法</u>：根据全局的语料统计<u>手动地</u>放置这些信息。
                        </p>
                    </div>

                    <p>一般地，该方法包括两个步骤 (过程如上图所示)：(1) 构造一个单词-上下文(Word-Context) 的关联矩阵; (2)
                        降低该矩阵的维度。这里降维主要有两个原因：一方面，原始矩阵非常大。另一方面，由于很多词语只出现在比较少见的上下文中，这样的矩阵可能有很多没什么信息量的元素（例如，空值）。
                    </p>

                    <div>
                        <img width="25%" src="../resources/lectures/word_emb/preneural/need_to_define-min.png"
                            style="float:right; margin-left:30px;" />

                        <p>要设计一个基于计数的方法，我们需要定义两件事：</p>
                        <ul>
                            <li>上下文的定义（包括单词出现在上下文中意味着什么）</li>
                            <li>关联的定义（即如何计算单词-上下文关联矩阵中每个元素）。</li>
                        </ul>
                    </div>

                    <p>下面我们介绍几种主流的方法来实现这两个概念。</p>
                    <br>


                    <h2 id="simple_cooccurrence"><b>简单：共现计数</b> <a
                            href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Simple%3A%20Co%2DOccurence%20Counts"
                            style="font-size:9px;" target="_blank">(英文原文)</a></h2>
                    <img width="31%" src="../resources/lectures/word_emb/preneural/define_simple-min.png"
                        style="float:right; margin-left:20px; margin-top:0px;" />

                    <img src="../resources/lectures/word_emb/preneural/window-min.png"
                        style="float:left; max-width:65%; margin-bottom:10px;" />

                    <p>在本方法中，上下文被定义为 L 大小窗口中的每个单词。而单词-上下文组成的 (w, c) 对应的关联矩阵元素是 w
                        在上下文 c 中出现的次数。这是获取词嵌入非常基本（且非常古老）的方法。</p>


                    <div class="card_with_ico">
                        <img class="ico" src="../resources/lectures/ico/bulb_empty.png" />
                        <div class="text_box_yellow">
                            <p class="data_text">
                                （曾经）著名的 HAL 模型（1996 年）也是这种方法的变体。在<a
                                    href="#research_improve_count_based">研究练习</a>中了解更多信息。
                            </p>
                        </div>
                    </div>


                    <h2><b>正点交互信息 (PPMI)</b> <a
                            href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Positive%20Pointwise%20Mutual%20Information%20(PPMI)"
                            style="font-size:9px;" target="_blank">(英文原文)</a></h2>
                    <img width="45%" src="../resources/lectures/word_emb/preneural/define_ppmi-min.png"
                        style="float:right; margin-left:20px;" />

                    <p>在这种方法中，上下文的定义和之前一样，但是单词和上下文之间关联矩阵的计算采用了更加聪明的PPMI (Positive
                        Pointwise Mutual Information, 正点交互信息) 度量。 PPMI
                        度量被广泛认为是神经网络出现前用于度量分布相似性的最佳技术。</p>


                    <div class="text_box_green">
                        <p class="data_text"><u>重要</u>：本方法与神经网络密不可分！事实证明，接下来介绍的一些基于神经网络的方法
                            (Word2Vec) 被证明实际上是在隐式逼近（移位的）PMI 矩阵的因式分解。敬请关注！</p>
                    </div>


                    <br>
                    <h2><b>潜在语义分析 (LSA)：理解文档</b> <a
                            href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Latent%20Semantic%20Analysis%20(LSA)%3A%20Understanding%20Documents"
                            style="font-size:9px;" target="_blank">(英文原文)</a></h2>

                    <img width="60%" src="../resources/lectures/word_emb/preneural/lsa-min.png"
                        style="float:right; margin-left:20px;" />
                    <p><a href="http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf">LSA (Latent Semantic Analysis, 潜在语义分析)
                            方法</a>
                        需要分析文档的集合。在前人方法中，上下文仅用于获取词向量，之后就会被丢弃。但在LSA方法中，上下文也要被充分利用，用来计算<b>文档向量</b>。也因此，LSA
                        成为最简单的主题模型之一：所获得的文档向量之间的余弦相似度可以用来衡量文档之间的相似度。</p>
                    <p>术语 LSA 有时是指将 SVD 应用于单词-文档矩阵的通用方法，其中单词-文档矩阵的各个元素可以通过不同的方式计算（例如，简单的共现、tf-idf
                        或其他的衡量方法).
                    </p>

                    <div class="text_box_green">
                        <p class="data_text"><u>动画预告！</u> <a
                                href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">LSA 的Wikipedia
                                主页</a>在单词-文档矩阵中有一个很好的动画用于揭示文档的主题探测过程，一定要看看！</p>
                    </div>


                </div>
                <br><br>

                <div id="word2vec">

                    <h1><b>Word2Vec：一种基于预测的方法</b> <a
                            href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Word2Vec%3A%20a%20Prediction%2DBased%20Method"
                            style="font-size:9px;" target="_blank">(英文原文)</a></h1>

                    <div id="w2v_idea">
                        <p>让我们再次回顾一下核心思想：</p>
                        <div class="green_left_thought">
                            <p class="data_text" style="font-size:18px;"><u>核心思想</u>：我们需要将有关上下文的信息引入词向量中。
                            </p>
                        </div>


                        <p>虽然基于计数的方法直观地实现了这个想法，但 Word2Vec 以不同的方式实现它：</p>
                        <div class="green_left_thought">
                            <p class="data_text" style="font-size:18px;"><u>方法</u>：通过<b>预测上下文</b>来<b>学习</b>好的词向量。
                                <br>Learn word vectors by teaching them to predict contexts.
                            </p>
                        </div>


                        <img width="250" src="../resources/lectures/word_emb/w2v/intro-min.png"
                            style="float:right; margin-left:20px;" />
                        <p>Word2Vec
                            是一个参数是词向量的模型。这些参数针对某个目标进行迭代优化。而该优化目标迫使词向量“知道”一个词可能出现的上下文：训练向量来预测相应词可能的上下文。正如在分布假设中所说的，如果向量“知道”了上下文，它们就能“知道”单词的含义。
                        </p>

                        <p>Word2Vec 是一种迭代方法。其核心思想如下：</p>
                        <ul>
                            <li>首先，先找一个巨大的文本语料库；</li>
                            <li>接着，使用滑动窗口浏览文本，每次移动一个单词。在每一步时，都会有一个中心词(Central
                                Word)和上下文词（Context Words, 即同一窗口中的其他词）；
                            </li>
                            <li>然后，计算上下文词在以此中心词作为条件下出现的概率；</li>
                            <li>最后，优化中心词向量以增加上述概率。</li>
                        </ul>
                        <br>

                        <img height="20" src="../resources/lectures/ico/paw_empty.png"
                            style="float:left; margin-top:-10px;" />
                        <div class="box_green_left">

                            <div class="text_box_green">
                                <p class="data_text">方法：浏览插图以了解主要思想。</p>
                            </div>

                            <div class="carousel"
                                data-flickity='{"imagesLoaded": true, "percentPosition": true, "selectedAttraction": 1, "friction": 1 }'
                                style="width:100%; height: auto; margin-top:10px; margin-bottom:30px; margin-left:10px;">
                                <div class="carousel-cell" style="width:100%">
                                    <center>
                                        <img width="600"
                                            src="../resources/lectures/word_emb/w2v/window_prob1-min.png" />
                                    </center>
                                </div>
                                <div class="carousel-cell" style="width:100%">
                                    <center>
                                        <img width="600"
                                            src="../resources/lectures/word_emb/w2v/window_prob2-min.png" />
                                    </center>
                                </div>
                                <div class="carousel-cell" style="width:100%">
                                    <center>
                                        <img width="600"
                                            src="../resources/lectures/word_emb/w2v/window_prob3-min.png" />
                                    </center>
                                </div>
                                <div class="carousel-cell" style="width:100%">
                                    <center>
                                        <img width="600"
                                            src="../resources/lectures/word_emb/w2v/window_prob4-min.png" />
                                    </center>
                                </div>
                                <div class="carousel-cell" style="width:100%">
                                    <center>
                                        <img width="600"
                                            src="../resources/lectures/word_emb/w2v/window_prob5-min.png" />
                                    </center>
                                </div>
                                <div class="carousel-cell" style="width:100%">
                                    <center>
                                        <img width="600"
                                            src="../resources/lectures/word_emb/w2v/window_prob6-min.png" />
                                    </center>
                                </div>
                            </div>

                            <p class="data_text" style="color:#aaa;font-size:14px;">
                                <u>编者按</u> 可视化的想法主要来自
                                <a href="http://web.stanford.edu/class/cs224n/index.html#schedule" target="_blank">斯坦福
                                    CS224n 课程</a>
                            </p>

                        </div>
                        <img height="20" src="../resources/lectures/ico/paw_empty.png"
                            style="float:left; margin-top:-10px;" />

                    </div>

                    <br><br>

                    <div id="w2v_objective_function">
                        <h2><b><u>目标函数</u>：负对数似然</b> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Objective%20Function%3A%20Negative%20Log%2DLikelihood"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h2>

                        <p>
                            对于文本语料库中的每个位置 \(t =1, \dots, T\), Word2Vec 在给定中心词的 m 大小窗口内预测上下文词
                            \(\color{#88bd33}{w_t}\):
                            \[\color{#88bd33}{\mbox{Likelihood}} \color{black}= L(\theta)=
                            \prod\limits_{t=1}^T\prod\limits_{-m\le j \le m, j\neq
                            0}P(\color{#888}{w_{t+j}}|\color{#88bd33}{w_t}\color{black}, \theta), \]
                            \(\theta\) 是需要被优化的参数（即词向量）。

                            目标函数（又名损失函数或成本函数） \(J(\theta)\) 则是平均负对数似然：
                        </p>


                        <center>
                            <img src="../resources/lectures/word_emb/w2v/loss_with_the_plan-min.png"
                                style="max-width:90%; margin-bottom:10px;" />
                        </center>

                        <p>请注意损失与上面核心思想的吻合程度：使用滑动窗口浏览文本并计算概率。
                            现在让我们来看看该如何计算这些概率。</p>


                        <img height="240" src="../resources/lectures/word_emb/w2v/two_vocs_with_theta-min.png"
                            style="float:right; margin-left: 25px; max-width:60%" />
                        <h3 id="word2vec_calculate_p"><b><u>如何计算</u></b>
                            \(P(\color{#888}{w_{t+j}}\color{black}|\color{#88bd33}{w_t}\color{black}, \theta)\)? <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=compute%20these%20probabilities.-,How%20to%20calculate,-P(w"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h3>

                        <p>对于每个单词 \(w\) 我们有两个向量:</p>
                        <ul>
                            <li>当它是一个中心词时，用\(\color{#88bd33}{v_w}\) 表示该词</li>
                            <li>当它是一个上下文词时，用\(\color{#888}{u_w}\) 表示该词</li>
                        </ul>
                        <p>（在训练结束后，我们通常会丢弃上下文词向量，仅使用中心词向量作为一个词的向量表征。）</p>


                        <p>于是，对于中心词 \(\color{#88bd33}{c}\) 和
                            上下文词 \(\color{#888}{o}\)，上下文词在该中心词的窗口中出现的概率是</p>
                        <img src="../resources/lectures/word_emb/w2v/prob_o_c-min.png" height="120"
                            style="margin: 10px;">


                        <br>

                        <details>
                            <summary>
                                <p><u>注意</u>：上式实际上就是<b>softmax函数</b>！（点击查看详情）</p>
                            </summary>

                            <p>softmax 函数定义如下：</font>:
                                \[softmax(x_i)=\frac{\exp(x_i)}{\sum\limits_{j=i}^n\exp(x_j)}.\]
                                <font face="arial">softmax 会映射任意值 \(x_i\) 到概率分布 \(p_i\):
                            </p>
                            <ul>
                                <li>
                                    <font face="arial">"max" 因为最大的 \(x_i\) 将有最大的概率 \(p_i\);
                                </li>
                                <li>
                                    <b>"soft"</b> 因为所有概率都不为零。
                                </li>
                            </ul>
                        </details>
                        <p>您将在自然语言（以及一般的深度学习）课程中大量地使用到此函数。</p>
                        <br>

                        <img height="20" src="../resources/lectures/ico/paw_empty.png"
                            style="float:left; margin-top:-10px;" />
                        <div class="box_green_left">

                            <div class="text_box_green">
                                <p class="data_text"><u>如何</u> 仔细阅读插图。请注意，对于同一个词，当它分别用作
                                    <font color="#88bd33">中心词</font>和<font color="#888">上下文词</font>时，使用的是不同的向量.
                                    例如, 在第一页中 <strong>a</strong> 是中心词，所以我们使用
                                    \(\color{#88bd33}{v_a}\)来表示它，当在第二页中它被用作上下文，此时我们使用
                                    \(\color{#888}{u_a}\) 来表示它。
                                </p>
                            </div>

                            <div class="carousel"
                                data-flickity='{ "imagesLoaded": true, "percentPosition": true, "selectedAttraction": 1, "friction": 1 }'
                                style="width:100%; height: auto; margin-top:10px; margin-bottom:30px; margin-left:10px;">
                                <div class="carousel-cell" style="width:100%">
                                    <center>
                                        <img width="600"
                                            src="../resources/lectures/word_emb/w2v/window_two_vocs1-min.png" />
                                    </center>
                                </div>
                                <div class="carousel-cell" style="width:100%">
                                    <center>
                                        <img width="600"
                                            src="../resources/lectures/word_emb/w2v/window_two_vocs2-min.png" />
                                    </center>
                                </div>
                                <div class="carousel-cell" style="width:100%">
                                    <center>
                                        <img width="600"
                                            src="../resources/lectures/word_emb/w2v/window_two_vocs3-min.png" />
                                    </center>
                                </div>
                                <div class="carousel-cell" style="width:100%">
                                    <center>
                                        <img width="600"
                                            src="../resources/lectures/word_emb/w2v/window_two_vocs4-min.png" />
                                    </center>
                                </div>
                                <div class="carousel-cell" style="width:100%">
                                    <center>
                                        <img width="600"
                                            src="../resources/lectures/word_emb/w2v/window_two_vocs5-min.png" />
                                    </center>
                                </div>
                                <div class="carousel-cell" style="width:100%">
                                    <center>
                                        <img width="600"
                                            src="../resources/lectures/word_emb/w2v/window_two_vocs6-min.png" />
                                    </center>
                                </div>
                            </div>

                        </div>
                        <img height="20" src="../resources/lectures/ico/paw_empty.png"
                            style="float:left; margin-top:-10px;" />

                    </div>
                    <br><br>

                    <div id="w2v_training">

                        <h2><b><u>如何训练</u>：梯度下降, 一次一词</b> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=instead.-,How%20to%20train,-%3A%20by%20Gradient%20Descent"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h2>

                        <p>让我们回想一下，对于词汇表中的任一单词，Word2Vec 模型的参数 \(\theta\) 都有两个向量对应，分别是向量
                            \(\color{#88bd33}{v_w}\) 和 \(\color{#888}{u_w}\)。
                            这些向量可以通过梯度下降优化训练目标来学习（需要指定学习率 \(\alpha\))：
                            \[\theta^{new} = \theta^{old} - \alpha \nabla_{\theta} J(\theta).\]</p>

                        <h3><b><u>一次一词</u></b> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=%CE%B8).-,One%20word%20at%20a%20time,-We%20make%20these"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h3>
                        <p>每一次模型优化时，我们都会更新一次模型参数，而每次更新都只针对一个中心词和它的一个上下文词。回顾一下损失函数：
                            \[\color{#88bd33}{\mbox{Loss}}\color{black} =J(\theta)= -\frac{1}{T}\log L(\theta)=
                            -\frac{1}{T}\sum\limits_{t=1}^T
                            \sum\limits_{-m\le j \le m, j\neq 0}\log
                            P(\color{#888}{w_{t+j}}\color{black}|\color{#88bd33}{w_t}\color{black}, \theta)=
                            \frac{1}{T} \sum\limits_{t=1}^T
                            \sum\limits_{-m\le j \le m, j\neq 0} J_{t,j}(\theta). \]

                            对于中心词 \(\color{#88bd33}{w_t}\), 每个上下文词损失函数中都包含项
                            \(J_{t,j}(\theta)=-\log
                            P(\color{#888}{w_{t+j}}\color{black}|\color{#88bd33}{w_t}\color{black},
                            \theta)\)
                            \(\color{#888}{w_{t+j}}\).

                            仔细看一下这一项，就可以知道该如何对此步骤进行更新。举个例子，假设我们有一个句子</p>
                        <center>
                            <img src="../resources/lectures/word_emb/w2v/sent_cat_central-min.png"
                                style="max-width:70%" />
                        </center>

                        <p>这个句子里明显可以看到有一个中心词 cat 和四个上下文词 cute, grey, playing 和 in。
                            由于一次只看一个词，我们将只选择一个上下文词，下面将以 cute 为例。 那么仅包含中心词 cat 和上下文词
                            cute 的损失项即可写成：

                            \[ J_{t,j}(\theta)= -\log
                            P(\color{#888}{cute}\color{black}|\color{#88bd33}{cat}\color{black}) =
                            -\log \frac{\exp\color{#888}{u_{cute}^T}\color{#88bd33}{v_{cat}}}{
                            \sum\limits_{w\in Voc}\exp{\color{#888}{u_w^T}\color{#88bd33}{v_{cat}} }} =
                            -\color{#888}{u_{cute}^T}\color{#88bd33}{v_{cat}}\color{black}
                            + \log \sum\limits_{w\in
                            Voc}\exp{\color{#888}{u_w^T}\color{#88bd33}{v_{cat}}}\color{black}{.}
                            \]</p>

                        <p> 此步骤中，哪些参数会被更新呢？</p>
                        <ul>
                            <li>中心词向量中，被更新的仅有 \(\color{#88bd33}{v_{cat}}\);</li>
                            <li>上下文词向量中，词汇表中所有单词的表示 \(\color{#888}{u_w}\) 都会被更新。</li>
                        </ul>

                        <p>下面是此步骤推导的示意图。</p>

                        <center>
                            <img src="../resources/lectures/word_emb/w2v/one_step_alg-min.png"
                                style="max-width:90%; margin-top:15px; margin-bottom:0px;" />
                            <!-- <p>翻译: 1. 取 \(\color{#88bd33}{v_{cat}}\ 和所有 \(\color{#888}{u}\ 的点积 </p>
                            <p>2. 指数化 3. 求和 4. 得到当前步的损失 5. 估计梯度，更新参数</p>
                            </p> -->
                        </center>

                        <br><br>

                        <img height="150" src="../resources/lectures/word_emb/w2v/loss_intuition-min.png"
                            style="float:right; margin-left: 25px; max-width:60%" />

                        <p>直观地，上述公式过程会通过参数更新以最小化 \(J_{t,j}(\theta)\)，本次更新可以让
                            \(\color{#88bd33}{v_{cat}}\) 和 \(\color{#888}{u_{cute}}\) 变得更相似（点积意义上的），并同时降低
                            \(\color{#88bd33}{v_{cat}}\) 和词汇表内所有其他单词 \(\color{#888}{u_{w}}\)的相似度。</p>

                        <p>
                            这听起来可能有点奇怪：如果其他词中也包含有效的上下文词（例如，示例句中的 grey, playing 和
                            in），为什么我们要降低\(\color{#88bd33}{v_{cat}}\)和<u>所有</u>其它单词之间的相似性？
                            <br>
                            <!-- Qian: 增加了最后一句解释-->
                            别担心：因为我们会更新每个上下文词（当然还有所有中心词），<u>平均下来</u>我们的词向量将学习到所有可能上下文的分布，即意味着本次对有效上下文词的降低会在其他的样例中被补偿。
                        </p>


                        <div class="card_with_ico">
                            <img class="ico" src="../resources/lectures/ico/dumpbell_empty.png" />
                            <div class="text_box_green">
                                <p class="data_text"><u>思维练习</u>：试试推导一下最后一步的梯度吧！如果不太会推导，请看一下论文
                                    <a href="https://arxiv.org/pdf/1411.2738.pdf">Word2Vec Parameter Learning Explained
                                    </a>。
                                </p>
                            </div>
                        </div>

                    </div>
                    <br>

                    <div id="w2v_negative_sampling">
                        <h2><b>更快的训练：负采样</b> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Faster%20Training%3A%20Negative%20Sampling"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h2>

                        <p>
                            在上面的示例中，对于每对中心词及其上下文词组成的训练样本，我们必须更新所有上下文词的向量。这无疑是非常低效的，因为每一步进行更新所需时间与总的词汇表大小成正比。</p>


                        <p>
                            但是，为什么我们必须在每一步都考虑词汇表中所有上下文词的向量呢？假设还是在刚才的示例中，我们考虑的并不是所有上下文单词的向量，而是只有目标上下文词（cute）和几个随机选择的其他单词，就像下图：
                        </p>

                        <center>
                            <img src="../resources/lectures/word_emb/w2v/negative_sampling-min.png"
                                style="max-width:90%; margin-top:15px; margin-bottom:15px;" />
                            <!-- <p>翻译: 左侧需降低<u>全部其他</u> \(\color{#888}{u}\ ，而右侧只降低<u>随机选择</u>的
                                \(\color{#888}{u}\ </p> -->
                        </center>

                        <br>
                        <!--    <img height="150" src="../resources/lectures/word_emb/w2v/loss_intuition_neg_sam-min.png"
         style="float:right; margin-left: 25px; max-width:60%"/> -->

                        <p>和以前一样，参数的更新增加了 \(\color{#88bd33}{v_{cat}}\) 和 \(\color{#888}{u_{cute}}\) 两者之间的相似性。
                            不同的是，现在我们只降低了
                            \(\color{#88bd33}{v_{cat}}\)与<u>K 个负样本子集</u>单词上下文向量的相似性，而非<u>所有</u>单词。
                        </p>

                        <p>
                            由于我们有一个庞大的语料库，平均下来我们将更新每个向量足够的次数，并且向量仍然能够很好地学习单词之间的关系。</p>

                        <p>
                            正式地，此步骤的新损失函数即：
                            \[ J_{t,j}(\theta)=
                            -\log\sigma(\color{#888}{u_{cute}^T}\color{#88bd33}{v_{cat}}\color{black}) -
                            \sum\limits_{w\in \{w_{i_1},\dots,
                            w_{i_K}\}}\log\sigma({-\color{#888}{u_w^T}\color{#88bd33}{v_{cat}}}\color{black}),
                            \]
                            \(w_{i_1},\dots, w_{i_K}\) 是这一步的K个负样本
                            ，\(\sigma(x)=\frac{1}{1+e^{-x}}\) 是sigmoid激活函数.</p>

                        <p>注意，
                            \(\sigma(-x)=\frac{1}{1+e^{x}}=\frac{1\cdot e^{-x}}{(1+e^{x})\cdot e^{-x}} =
                            \frac{e^{-x}}{1+e^{-x}}= 1- \frac{1}{1+e^{x}}=1-\sigma(x)\). 于是loss函数可以被写成：
                            \[ J_{t,j}(\theta)=
                            -\log\sigma(\color{#888}{u_{cute}^T}\color{#88bd33}{v_{cat}}\color{black}) -
                            \sum\limits_{w\in \{w_{i_1},\dots,
                            w_{i_K}\}}\log(1-\sigma({\color{#888}{u_w^T}\color{#88bd33}{v_{cat}}}\color{black})).
                            \]</p>

                        <div class="card_with_ico">
                            <img class="ico" src="../resources/lectures/ico/dumpbell_empty.png" />
                            <div class="text_box_green">
                                <!-- Qian: 加了一个前缀-->
                                <p class="data_text"><u>思维练习</u>：使用负采样时梯度和更新如何变化？</p>
                            </div>
                        </div>

                        <br>
                        <h3 id="choice_of_neg_examples"><b><u>负样本的选择</u></b> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=The%20Choice%20of%20Negative%20Examples"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h3>
                        <p>每个单词只有少数“真正”的上下文。因此，随机选择的词大概率是“否定的”，即不是真正的上下文。这个简单的想法不仅可以有效地训练
                            Word2Vec，还可以用于许多其他应用中，其中一些我们将在后面的课程中看到。</p>


                        <p>一般地，Word2Vec 根据词的先验分布随机抽取负样本。
                            假设 \(U(w)\) 是单词出现的概率分布, 一般可以用单词在文本语料库中的频率近似计算。Word2Vec
                            修改了这个分布以更频繁地采样到频率较低的单词，最终它选择 \(U^{3/4}(w)\) 进行负样本单词的采样.</p>


                    </div>

                    <div id="w2v_skipgram_cbow">
                        <h2><b>Word2Vec 变体：Skip-Gram 和 CBOW</b> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=.-,Word2Vec%20variants%3A%20Skip%2DGram%20and%20CBOW,-There%20are%20two"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h2>

                        <p>Word2Vec 有两种变体：Skip-Gram 和 CBOW。</p>


                        <p>Skip-Gram 就是上文所介绍的模型：给定中心词，预测上下文词。目前，带有负采样的 Skip-Gram
                            是最流行的方法。</p>

                        <p>CBOW（Continuous Bag-of-Words，连续词袋模型）则反其道而行之：给定上下文词，预测中心词。 一般来说 CBOW
                            直接使用上下文词向量的加和来预测，这个加和后的向量也被称为“词袋”。</p>

                        <center>
                            <img src="../resources/lectures/word_emb/w2v/cbow_skip-min.png"
                                style="max-width:90%; margin-top:15px; margin-bottom:15px;" />
                        </center>


                        <div class="card_with_ico">
                            <img class="ico" src="../resources/lectures/ico/dumpbell_empty.png" />
                            <div class="text_box_green">
                                <p class="data_text"><u>思维练习</u>：CBOW 模型的损失函数和梯度如何推导？如果推不出来，你可以再看一下
                                    <a href="https://arxiv.org/pdf/1411.2738.pdf">Word2Vec Parameter Learning Explained
                                    </a>
                                    这篇论文。
                                </p>
                            </div>
                        </div>

                    </div>

                    <div id="w2v_additional_notes">
                        <h2><b>附加笔记</b> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Learning%20Explained.-,Additional%20Notes,-The%20original%20Word2Vec"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h2>

                        <p>原始 Word2Vec 论文是这两篇：</p>
                        <ul>
                            <li><a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank">Efficient Estimation of
                                    Word
                                    Representations in Vector Space</a></li>
                            <li>
                                <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"
                                    target="_blank">
                                    Distributed Representations of Words and Phrases and their Compositionality</a>
                            </li>
                        </ul>
                        <p>你可以通过阅读原文以获取有关实验、实现和超参数的详细信息。本文将提供一些你需要知道的最重要的事。</p>


                        <h3><u>这并非一个全新的idea</u> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=need%20to%20know.-,The%20Idea%20is%20Not%20New,-The%20idea%20to"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h3>

                        <p>请注意，学习词向量（分布式表示）的想法并不新鲜。例如，有人尝试将词向量作为更大网络的一部分来学习，然后提取出模型的嵌入层作为词向量。有关前人方法的相关信息，可以通过阅读
                            Word2Vec 原论文摘要获取。</p>

                        <p>Word2Vec 让人出乎意料的是，它能够在庞大的数据集和大型词汇表上非常快地学习到高质量的词向量。 当然，我们将在<a
                                href="#analysis_interpretability">分析与解释部分</a>所要介绍的一些有趣的属性让 Word2Vec
                            闻名遐迩。</p>


                        <h3><u>为什么要设计两个向量？</u> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Word2Vec%20very%20famous.-,Why%20Two%20Vectors%3F,-As%20you%20remember"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h3>
                        <p>正如上文所介绍的，在 Word2Vec 中，我们要为每个单词训练两个向量：一个是中心词向量，另一个是上下文词向量。
                            训练后，上下文词向量就被丢掉了。</p>

                        <p>那既然训练后只用到中心词向量，为什么训练的时候要搞两个向量呢？其实，这正是使 Word2Vec
                            如此简洁的重要技巧之一！回顾一下损失函数：
                            \[ J_{t,j}(\theta)=
                            -\color{#888}{u_{cute}^T}\color{#88bd33}{v_{cat}}\color{black} -
                            \log \sum\limits_{w\in V}\exp{\color{#888}{u_w^T}\color{#88bd33}{v_{cat}}}\color{black}{.}
                            \]
                            当每个单词使用不同的向量来分别表示其作为中心词和上下文词的表征时，损失函数的第一项和指数内的点积对于参数都是线性的（因为两项互不相关）。因此，梯度的计算将异常容易。</p>

                        <div class="card_with_ico">
                            <img class="ico" src="../resources/lectures/ico/dumpbell_empty.png" />
                            <div class="text_box_green">
                                <p class="data_text"><u>思维练习</u>：对单词仅使用一个向量时的情况进行推导（包括损失和梯度）
                                    (\(\forall w \ in \ V, \color{#88bd33}{v_{w}}\color{black}{ = }\color{#888}{u_{w}}\)
                                    ).
                                </p>
                            </div>
                        </div>

                        <div class="card_with_ico">
                            <img class="ico" width="40" src="../resources/lectures/ico/book_empty.png" />
                            <div class="text_box_pink">
                                <p class="data_text">
                                    虽然Word2Vec的标准做法是丢弃上下文词向量，但也有论文指出一起用中心词和上下文词向量可能效果更好。查看
                                    <a href="#papers_good_old_classics">这里</a> 获取更多细节。
                                </p>
                            </div>
                        </div>

                        <h3><u>更好的训练</u> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=details%20are%20here.-,Better%20training,-There%27s%20one%20more"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h3>

                        <div class="card_with_ico">
                            <img class="ico" src="../resources/lectures/ico/bulb_empty.png" />
                            <div class="text_box_yellow">
                                <!-- Qian: 统一了思维练习的格式 -->
                                <p class="data_text">小技巧：从<a href="#w2v_subsample_frequent">研究练习</a>中了解更多。</p>
                            </div>
                        </div>


                        <h3><u>与 PMI 矩阵分解的关系</u> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Relation%20to%20PMI%20Matrix%20Factorization"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h3>

                        <div class="card_with_ico">
                            <img class="ico" width="40" src="../resources/lectures/ico/book_empty.png" />
                            <div class="text_box_pink">
                                <p class="data_text">
                                    Word2Vec SGNS (Skip-Gram with Negative Sampling，带负采样的Skip-Gram) 其实是在隐式地逼近（移位的）PMI
                                    矩阵的因式分解。
                                    点击<a href="#papers_good_old_classics">这里</a>了解更多。
                                </p>
                            </div>
                        </div>

                        <h3><u>窗口大小的影响</u> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=The%20Effect%20of%20Window%20Size"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h3>

                        <p>实践表明，滑动窗口的大小对得到的向量相似度有很大的影响。 例如，论文<a href="https://arxiv.org/pdf/1510.00726.pdf">A Primer on
                                Neural Network Models for Natural
                                Language Processing</a>指出，较大的窗口往往会产生更多语义主题上的相似性（即dog, bark, leash 或
                            walked, run, walking），而较小的窗口往往会产生更多功能和句法上的相似性（即 Poodle, Pitbull,
                            Rottweiler, 或 walking, running, approaching）。</p>

                        <h3><u>标准超参数</u> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=(Somewhat)%20Standard%20Hyperparameters"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h3>
                        与往常一样，超参数的选择通常取决于需要解决的任务，你可以查看原始论文以获取更多详细信息。
                        <br><br>

                        标准设定
                        <ul>
                            <li>模型：带负采样的 Skip-Gram;</li>
                            <li>负样本数量：对于较小的数据集，15-20；对于大型数据集（通常使用），只需 2-5 即可。</li>
                            <li>词向量的维度：常用的值为 300，但其他变体（例如，100 或 50）也可以。有关最佳维度的理论解释，请查看<a
                                    href="#related_papers">相关论文</a>部分。
                            </li>
                            <li>滑动窗口（即上下文）大小：5-10。</li>
                        </ul>

                        <br>
                    </div>

                </div>

                <div id="glove">
                    <h1><b>GloVe：单词表示的全局向量</b> <a
                            href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=GloVe%3A%20Global%20Vectors%20for%20Word%20Representation"
                            style="font-size:9px;" target="_blank">(英文原文)</a></h1>

                    <center>
                        <img src="../resources/lectures/word_emb/glove/idea-min.png"
                            style="max-width:90%; margin-bottom:15px;" />
                    </center>

                    <div>
                        <p><a href="https://www.aclweb.org/anthology/D14-1162.pdf">GloVe 模型</a>是基于计数的方法和基于预测的方法（例如
                            Word2Vec）的组合。模型名称 GloVe 代表 Global Vectors，也体现了它的核心思想：利用语料库中的全局信息来学习词向量。
                        </p>

                        <p><a href="#simple_cooccurrence">正如之前所见</a>，最简单的基于计数的方法使用共现计数来衡量单词 w
                            和上下文 c 之间的关联：N(w, c)。类似地，GloVe 也使用这种计数来构建损失函数：</p>
                        <center>
                            <img src="../resources/lectures/word_emb/glove/glove_loss-min.png"
                                style="max-width:80%; margin-bottom:15px;" />
                            <!-- <p>翻译: 上下文向量，词向量，偏置项（也是学出来的）</p>
                            <p>权重函数 f(x) 用于：1. 惩罚较小的 N(w, c)；2. 但也不过于注重较大的 N(w, c)</p> -->
                        </center>

                        <p>与 Word2Vec 类似，Glove 也有中心词向量和上下文词向量的区分，这二者构成了Glove的参数。
                            此外，该方法对每个词向量都引入了一个标量偏置项 (即上图的 b)。</p>

                        <p>
                            有趣的地方在于，GloVe 控制了少见词和频繁词的影响：对每对 (w, c) 而言，损失将以如下方式加权：</p>
                        <ul>
                            <li>对于罕见的 (w, c)，损失会受惩罚，</li>
                            <li>对于频繁出现的 (w, c)，损失也不会被过度加权。</li>
                        </ul>

                        <p class="data_text" style="color:#888;"><u>编者按</u>：这个损失函数看起来很自然，但<a
                                href="https://www.aclweb.org/anthology/D14-1162.pdf">GloVe
                                原文</a>有很好的动机来推导得到上述公式。本文不提供详细说明，但您可以自己阅读，真的非常好！</p>
                    </div>
                </div>
                <br>


                <div id="evaluation">
                    <h1><b>词嵌入的评价</b> <a
                            href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Evaluation%20of%20Word%20Embeddings"
                            style="font-size:9px;" target="_blank">(英文原文)</a></h1>

                    <p>
                        我们如何评价一种获取词嵌入的方法要比另一种更好呢？目前学术界有两种评估手段（不仅适用于词嵌入）：内在评价和外在评价。</p>
                    <!-- Qian: 加入新的编者按，解释过渡一下-->
                    <p class="data_text" style="color:#888;"><u>编者按</u>：下文中我们使用词嵌入 (Word Embeddings) 来指代词向量
                        (Word Vectors)，但实际上它们是一个东西。</p>

                    <div>
                        <h3><b><u>内在评价</u></b>：基于内在属性 <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Intrinsic%20Evaluation%3A%20Based%20on%20Internal%20Properties"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h3>
                        <img width="40%" src="../resources/lectures/word_emb/intrinsic_evaluation-min.png"
                            style="float:right; margin-left: 25px; max-width:60%" />
                        <p>这种类型的评价着眼于词嵌入的内在属性，即它们捕捉单词“含义”的程度。在<a
                                href="#analysis_interpretability">分析与解释部分</a>，我们将详细讨论如何通过词相似性和词类比任务上评价词嵌入。
                        </p>
                    </div>

                    <br>

                    <div>
                        <h3><b><u>外部评价</u></b>：基于下游任务 <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Extrinsic%20Evaluation%3A%20On%20a%20Real%20Task"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h3>
                        <img width="60%" src="../resources/lectures/word_emb/extrinsic_evaluation-min.png"
                            style="float:right; margin-left: 25px; max-width:60%" />

                        <p>这种类型的评价会告诉读者哪些嵌入更适合读者真正关心的任务（例如，文本分类、指代消解等）。
                            <br><br>
                            在这种情况下，你必须为下游任务制定一个可以搭配不同词嵌入方法的模型。然后，通过模型的性能来确定哪些词嵌入方法更好。
                        </p>
                    </div>

                    <br>

                    <div>
                        <h3><b><u>如何选择</u></b> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=embeddings%20are%20better.-,How%20to%20Choose%3F,-One%20thing%20you"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h3>
                        <img width="60%" src="../resources/lectures/word_emb/evaluation_tradeoff-min.png"
                            style="float:right; margin-left: 25px; max-width:60%" />
                        <p>你必须要接受的是，没有完美的词嵌入，也没有适用于所有情况的词嵌入：它总是取决于很多事情。</p>

                        <p>关于评价，您通常更关心下游任务本身的性能。 因此，读者可能会对外部评价更感兴趣。
                            然而，下游任务上的模型通常需要大量的时间和资源来训练，尤其是当你需要训练多个的时候，训练成本可能过于昂贵。</p>
                        <p>至于选择哪个词嵌入，最后还是要取决于读者自己 :)</p>
                    </div>

                </div>
                <br><br>

                <div id="analysis_interpretability">
                    <img height="40" src="../resources/lectures/ico/analysis_empty.png"
                        style="float:left; padding-right:20px; " />
                    <h1><b>分析与解释</b> <a
                            href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=call%20to%20make%20%3A)-,Analysis%20and%20Interpretability,-Lena%3A%20For"
                            style="font-size:9px;" target="_blank">(英文原文)</a></h1>

                    <p class="data_text" style="color:#888;"><u>编者按</u>：对于词嵌入，本节的大部分内容通常被认为是内在评价的手段之一。
                        但是，由于理解模型学到的内容（不是指特定任务上的性能）是人们通常为分析所做的事情，因此有了本分析部分。
                    </p>


                    <div id="analysis_walk_through_space">
                        <h2>漫步太空……语义空间！ <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Take%20a%20Walk%20Through%20Space...%20Semantic%20Space!"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h2>

                        <p>
                            语义空间旨在创建捕捉单词含义的自然语言表征。词嵌入构成了语义空间，所以我们一般会将多维空间中的一组词向量称作“语义空间”。</p>

                        <p>下面显示了在 twitter 数据（取自 <a href="https://github.com/RaRe-Technologies/gensim-data"
                                target="_blank">gensim</a>）上训练的 GloVe 向量形成的语义空间。
                            使用 t-SNE 将向量投影到二维空间，下图中展示了前3000个最常用的单词。</p>


                        <img height="20" src="../resources/lectures/ico/paw_empty.png"
                            style="float:left; margin-top:-10px;" />
                        <div class="box_green_left">
                            <div class="text_box_green">

                                <p class="data_text">
                                    <u>怎么做</u>：请遍历语义空间并尝试找到
                                </p>
                                <ul class="data_text" style="padding-right:10px;">
                                    <li>语言集群：西班牙语、阿拉伯语、俄语、英语。 还能找到更多的语言吗？</li>
                                    <li>其他集群：食物、家庭、姓名、地理位置。还能找到什么？</li>
                                </ul>
                            </div>

                            <center>
                                <iframe frameborder="0" width="510" height="510" scrolling="no"
                                    src="../resources/lectures/word_emb/analysis/glove100_twitter_top3k.html">
                                </iframe>
                            </center>

                            <!--<p class="data_text" style="color:#aaa;font-size:13px;"><u>Lena:</u> Embeddings are from
    <a href="https://github.com/RaRe-Technologies/gensim-data">gensim.</a></p>-->

                        </div>
                        <img height="20" src="../resources/lectures/ico/paw_empty.png"
                            style="float:left; margin-top:-10px;" />

                    </div>
                    <br><br>

                    <div id="analysis_neighbors">
                        <h2><b>寻找最近的邻居（最近邻）</b> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=can%20you%20find%3F-,Nearest%20Neighbors,-The%20example%20is"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h2>

                        <p style="text-align: center; float: right; display: block; margin-left:25px;
         margin-top:-10px; max-width:50%;">
                            <img src="../resources/lectures/word_emb/analysis/frog-min.png" alt="" /><br />
                            <span style="font-size: small;">样例取自 <a href="https://nlp.stanford.edu/projects/glove/"
                                    target="_blank">GloVe 项目主页</a>.</span>
                        </p>

                        <p>在浏览语义空间的过程中，可能读者会注意到某些点与它的邻居通常密切相关。有时，即使是罕见词也有类似性质。请看右图中的例子：语义空间中
                            leptodactylidae (细趾蟾科) 或 litoria (雨滨蛙属) 这两个罕见词与 frogs (青蛙) 的距离很近。</p>

                        <br>
                        <br>
                        <br>

                        <p style="text-align: center; float: right; display: block; margin-left:25px; max-width:30%;">
                            <img src="../resources/lectures/word_emb/analysis/rare_words-min.png" alt="" /><br />
                            <span style="font-size: small;"> 许多示例都来自
                                <a href="https://nlp.stanford.edu/~lmthang/data/papers/conll13_morpho.pdf"
                                    target="_blank">
                                    Rare Words similarity benchmark</a>。</span>
                        </p>

                        <h3><u>词相似度的测试基准</u> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Word%20Similarity-,Benchmarks,-%22Looking%22%20at%20nearest"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h3>
                        <p>
                            通过余弦相似度或欧几里得距离“查看”最近的邻居是评价模型所学习到的词嵌入质量的常用方法之一。学术界中已经有一些单词相似度的测试基准（即测试集），它们由人类判断有相似性的词对组成。词嵌入的质量可以用这些词对的相似度分数（分别来自模型和人类）之间的相关性所衡量。
                        </p>

                    </div>

                    <br>
                    <br>

                    <div id="analysis_linear_structure">
                        <h2><b>线性结构</b> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=and%20from%20humans).-,Linear%20Structure,-While%20similarity%20results"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h2>

                        <p>尽管词语相似度的结果令人鼓舞，但它们并不令人惊讶：因为词嵌入是专门训练来反映单词相似度的。真正令人惊讶的是，词之间的许多<u>语义和句法关系</u>在词向量空间中（几乎）是线性的。
                        </p>

                        <img src="../resources/lectures/word_emb/analysis/king_example-min.png" alt=""
                            style="float:right; width: 50%; margin-left: 20px; margin-top: 10px;" />


                        <p>例如，king (国王) 和 queen (王后) 之间的差距与 man (男人) 和 woman (女人) 之间的差距是类似的。
                            再例如，queens 是queen 的复数，正如同 kings 是 king 的复数。 man - woman \(\approx\) king - queen
                            的例子可能是最受欢迎的例子，但其实也有许多其他有趣的例子。</p>

                        <p>下面是 country-capital (国家-首都) 关系和一些句法关系的例子。</p>

                        <center>
                            <img src="../resources/lectures/word_emb/analysis/examples_both-min.png" alt=""
                                style="width: 100%; margin: 10px;" />
                        </center>

                        <div class="card_with_ico">
                            <img class="ico" width="40" src="../resources/lectures/ico/book_empty.png" />
                            <div class="text_box_pink">
                                <p class="data_text">
                                    在 ICML 2019 上，有论文表明 Word2Vec 中的类比实际上存在理论可以解释。<a href="#papers_theory">更多细节请看这里。</a>
                                    <br><br>
                                    <font color="#888"> <u>编者按</u>：<a
                                            href="https://arxiv.org/pdf/1901.09813.pdf">Analogies
                                            Explained: Towards Understanding Word Embeddings</a>这篇论文来自爱丁堡大学的 Carl
                                        Allen 和 Timothy Hospedales，在 ICML 2019 上获得了最佳论文荣誉奖——当之无愧！</font>>
                                </p>
                            </div>
                        </div>


                        <h3><u>词类比的测试基准</u> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Word%20Analogy-,Benchmarks,-These%20near%2Dlinear"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h3>

                        <p>上面所介绍的这些近似线性的关系启发了一种新型的评估方式：词类比评估 (Word Analogy Evaluation)。</p>

                        <center>
                            <img src="../resources/lectures/word_emb/analysis/analogy_task_v2-min.png" alt=""
                                style="width: 70%; margin: 10px;" />
                        </center>


                        <p style="text-align: center; float: right; display: block;
        margin-bottom:20px; margin-left:25px; max-width:60%;">
                            <img src="../resources/lectures/word_emb/analysis/analogy_testset-min.png" alt="" /><br />
                            <span style="font-size: small;">关系和词对的示例来自
                                <a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank">Google analogy
                                    测试集</a>.</span>
                        </p>

                        <p>给定相同关系的两个词对，例如 (man, woman) 和 (king, queen)，词类比任务是检查模型是否可以根据任三个词来找到目标词。例如，我们需要检查与
                            king - man + woman 最接近的向量是否对应于单词 queen。</p>


                        <p>学术界已经有一些类比基准，其中包括 (<a href="https://www.aclweb.org/anthology/N13-1090.pdf"
                                target="_blank">MSR</a> +
                            <a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank">Google analogy</a> 测试集) 和 <a
                                href="https://www.aclweb.org/anthology/N16-2002.pdf" target="_blank">BATS
                                (更大规模的类比测试集)</a>。
                        </p>

                    </div>
                    <br>

                    <div id="analysis_cross_lingual">
                        <h2><b>语言之间的相似性</b> <a
                                href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Similarities%20across%20Languages"
                                style="font-size:9px;" target="_blank">(英文原文)</a></h2>

                        <p>
                            我们可以看到单词之间的一些关系在语义空间中（几乎）都是线性的。那么，语言之间呢？事实证明，语言之间的语义空间也是（有点）线性的，这意味着你可以将一个语言的语义空间线性映射到另一个语言的语义空间，从而两种语言中语义相同的词在联合语义空间中可以匹配上。
                        </p>

                        <center>
                            <img src="../resources/lectures/word_emb/analysis/cross_lingual_matching-min.png" alt=""
                                style="width: 100%; margin: 10px;" />
                            <!-- <p>翻译：第 1 步 - 为每种语言都训练词嵌入；第 2 步 - 将一个语义空间线性地投影到另外一个语义空间，使得词典中的词对可以在新的语义空间中匹配；第
                                3 步 - 从语义空间中寻找一些新的匹配对，这些匹配对可以帮助你挖掘更多词对。</p> -->
                        </center>


                        <p>在Word2Vec提出之后不久，上图形象地解释了 <a href="https://arxiv.org/pdf/1309.4168.pdf" target="_blank">Tomas
                                Mikolov 等人提出的方法</a>。举个例子，我们得到一组词对及其向量表示
                            \(\{\color{#88a635}{x_i}\color{black}, \color{#547dbf}{z_i}\color{black} \}_{i=1}^n\),
                            其中 \(\color{#88a635}{x_i}\) 和 \(\color{#547dbf}{z_i}\)
                            分别是源语言中第 i 个单词及其在目标语言中对应单词的向量。
                            优化目标是找到一个变换矩阵 W，使得 \(W\color{#547dbf}{z_i}\) 近似于
                            \(\color{#88a635}{x_i}\)：即从源语言空间匹配目标语言空间。
                            \(W\) 可以通过梯度下降法来学习，最小化以下目标：
                            \[W = \arg \min\limits_{W}\sum\limits_{i=1}^n\parallel W\color{#547dbf}{z_i}\color{black} -
                            \color{#88a635}{x_i}\color{black}\parallel^2\]</p>

                        <p>
                            在原始论文中，用来学习的词汇由源语言中5000个最常用的单词及其对应翻译单词组成，其余的词对都是通过学习得到。</p>


                        <div class="card_with_ico">
                            <img class="ico" width="40" src="../resources/lectures/ico/book_empty.png" />
                            <div class="text_box_pink">
                                <p class="data_text">
                                    后来事实证明，上面所介绍的方法不需要初始的字典——即使我们对语言之间的对齐一无所知，也可以在语义空间之间建立映射！
                                    <a href="#papers_cross_lingual">更多细节请看这里。</a>
                                </p>
                            </div>
                        </div>

                        <div class="card_with_ico">
                            <img class="ico" width="40" src="../resources/lectures/ico/book_empty.png" />
                            <div class="text_box_pink">
                                <p class="data_text">
                                    语言之间的“真正”映射是线性的，还是更复杂呢？ 我们可以查看所学习到的语义空间的几何形状来确认。<a
                                        href="#papers_analyzing_geometry">更多细节请看这里。</a>
                                </p>
                            </div>
                        </div>

                        <div class="card_with_ico">
                            <img class="ico" src="../resources/lectures/ico/bulb_empty.png" />
                            <div class="text_box_yellow">
                                <p class="data_text">
                                    将不同的词嵌入空间线性投影以近似地匹配它们的想法可以用在很不一样的任务上！ 在<a
                                        href="#research_thinking">研究思考</a>部分可以了解更多信息。</p>
                            </div>
                        </div>

                    </div>

                </div>

                <br><br>


                <div id="research_thinking">
                    <img height="40" src="../resources/lectures/ico/bulb_empty.png"
                        style="float:left; padding-right:10px; margin-top:-20px;" />
                    <h1 style="margin-left:10px; margin-right:20px; float: left; margin-top:-20px"><b>研究思考</b> <a
                            href="https://lena-voita.github.io/nlp_course/word_embeddings.html#:~:text=Research-,Thinking,-How%20to"
                            style="font-size:9px;" target="_blank">(英文原文)</a></h1>
                    <hr color="#fced95" style="height:5px">
                    <br><br>


                    <fieldset style="border: 1px solid #f0e4a5;
    border-radius: 5px;">
                        <legend>
                            <p class="data_text"><strong>How to</strong></p>
                        </legend>
                        <ul class="data_text">
                            <li>阅读开头的简短描述——这是我们的出发点，一些已知的内容。

                            </li>
                            <li>读一个问题并思考：一分钟，一天，或是一周，给自己一些时间！即使你不是一直在想这个问题，你仍然得到一些思考和启发。

                            </li>
                            <li>看看可能的答案——之前回答/解决这个问题的尝试。
                                <br>
                                <u>重要提示：</u>
                                你<strong>不应该</strong>提出和这里完全一样的东西—记住，每篇论文通常需要花费作者几个月的工作。思考这些事情是一种有用的习惯！科学家所需要的盛夏的就是时间：用来不断地尝试-失败-思考，直到成功。

                            </li>
                        </ul>

                        <p class="data_text">众所周知，如果你不是马上得到答案，而是先思考一下，你会更容易学到东西。即使你不想成为一名研究人员，这仍然是一个学习的好方法！
                        </p>
                    </fieldset>


                    <br><br>


                    <!--##################################################-->
                    <!--##################################################-->
                    <!--##################################################-->

                    <div class="research_circle" style="float:left;"></div>
                    <h2 style="margin-top:-10px; float: left; padding-left:10px; padding-right:10px; color:#786702">
                        基于计数的方法</h2>
                    <div class="box_yellow_left">

                        <!--##################################################-->
                        <div class="researchCard" id="thumbnail_research">
                            <div class="researchIntro" id="research_improve_count_based">

                                <div class="cardContent">

                                    <div class="research_title">
                                        提升简单的共现计数
                                    </div>

                                    <hr color="#dedeca" style="margin:5px;">
                                    最简单的共现计数同等对待所有上下文词，尽管这些词与中心词的相对位置并不一样。
                                    例如，在右边这个例子中，中心词 cat 与各个上下文词 cute, grey, playing, in 的共现计数均为
                                    1。

                                </div>
                                <div>
                                    <!--<div class="research_tag">neural</div>-->

                                    <img src="../resources/lectures/word_emb/research/counts_simple-min.png" alt=""
                                        style="margin-top:20px;" class="center" />

                                </div>

                            </div>
                            <hr color="#dedeca" style="margin:5px">
                            <div class="cardContent">

                                <span class="research_question">?</span>
                                不同距离的上下文词是否同样重要？ 如果不是，我们该如何改进共现计数？<br>
                                <details>
                                    <summary class="research_summary">
                                        可能的答案
                                    </summary>


                                    <img src="../resources/lectures/word_emb/research/counts_modify_position-min.png"
                                        alt="" style="margin-left:20px; float:right; width: 40%" />

                                    直觉上，离中心词越近的上下文词越重要。例如，直接邻居能比距离为 3 的单词提供更多信息。
                                    <br><br>
                                    我们可以利用这个简单的想法来改进模型：在评估计数时，给更接近中心的词更多的权重。这个想法被用在了过去非常出名的<a
                                        href="https://link.springer.com/content/pdf/10.3758/BF03204766.pdf"
                                        target="_blank">HAL
                                        (1996)</a>模型中。研究者们正如右侧示例一样修改了计数方法。

                                </details>


                                <br>
                                <span class="research_question">?</span>
                                在语言中，词序很重要，比如说上下文词放在左侧或右侧时有不同的含义。我们该如何区分左右上下文？
                                <details>
                                    <summary class="research_summary">
                                        现有方法之一
                                    </summary>

                                    <img src="../resources/lectures/word_emb/research/counts_left_right-min.png" alt=""
                                        style="margin-left:20px; float:right; width: 50%" />

                                    在这里，上面说的加权想法是行不通的：我们不能说左侧或右侧哪个上下文更重要。<br><br>
                                    我们接下来要做的是分别评估左侧和右侧上下文的共现。
                                    对于每个上下文单词，我们引入有两个不同的计数函数：一个是左侧上下文，另一个是右侧上下文。
                                    这意味着共现矩阵变成了 |V| 行和 2|V| 列。 这个想法也被用于<a
                                        href="https://link.springer.com/content/pdf/10.3758/BF03204766.pdf"
                                        target="_blank">HAL
                                        model (1996)</a>。
                                    <br><br>
                                    看右图的例子：请注意，对于cute，仅统计其的左共现计数，对于cat 则仅统计其右共现计数。
                                </details>

                            </div>
                        </div>

                        <!--##################################################-->
                    </div>
                    <div class="research_circle" style="float:left;"></div>
                    <!--##################################################-->
                    <!--##################################################-->
                    <!--##################################################-->


                    <br><br>

                    <!--##################################################-->
                    <!--##################################################-->
                    <!--##################################################-->

                    <div class="research_circle" style="float:left;"></div>
                    <h2 style="margin-top:-10px; float: left; padding-left:10px; padding-right:10px; color:#786702">
                        Word2Vec</h2>
                    <div class="box_yellow_left">

                        <!--##################################################-->
                        <div class="researchCard" id="thumbnail_research">
                            <div class="researchIntro" id="w2v_subsample_frequent">

                                <div class="cardContent">

                                    <div class="research_title">
                                        所有上下文词对训练一样重要吗？
                                    </div>

                                    <hr color="#dedeca" style="margin:5px;">
                                    在 Word2Vec 训练期间，模型优化时会对每个上下文词进行更新。 例如，对于中心词
                                    cat，我们对每个上下文词进行更新：cute、grey、playing、in。

                                </div>
                                <div>
                                    <!--<div class="research_tag">neural</div>-->

                                    <img src="../resources/lectures/word_emb/research/cat_5windows-min.png" alt=""
                                        style="margin-top:20px;" class="center" />

                                </div>

                            </div>
                            <hr color="#dedeca" style="margin:5px">
                            <div class="cardContent">

                                <span class="research_question">?</span>
                                所有上下文词都同样重要吗？<br>
                                哪些词类型提供的信息比其他词多或少？什么特征可能会影响单词的重要性呢? 在看答案前请思考一下！
                                <details>
                                    <summary class="research_summary">
                                        可能的答案
                                    </summary>

                                    <ul>
                                        <li><u>词频</u>
                                            <br>
                                            我们可以预期，常见的词通常比罕见的词提供的信息更少。 例如，in 作为 cat
                                            的上下文中这一事实并不能告诉我们太多关于 cat 的含义：单词 in 可以是许多单词的上下文。
                                            相比之下，cute, grey 和 playing 更容易让我们猜测到中心词 cat。
                                        </li>
                                        <li>
                                            <u>和中心词的距离</u>
                                            <br>
                                            正如我们在之前在基于计数的方法的练习中所讨论的，更接近中心的单词可能更重要。
                                        </li>
                                    </ul>
                                </details>


                                <br>
                                <span class="research_question">?</span>
                                我们如何改进Word2Vec的训练？
                                <details>
                                    <summary class="research_summary">
                                        来自原始论文的技巧
                                    </summary>
                                    <h3><u>1. 词频</u></h3>

                                    <center>
                                        <img src="../resources/lectures/word_emb/w2v/freq_subsampling_idea-min.png"
                                            style="max-width:80%; margin-bottom:15px;" />
                                    </center>

                                    为了解释罕见词和常见词的不同信息量，Word2Vec 使用了一种简单的二次采样方法：每个训练集中的词
                                    \(w_i\) 都有一定概率被忽略，概率由如下公式计算
                                    \[P(w_i)=1 - \sqrt{\frac{thr}{f(w_i)}}\]
                                    其中 \(f(w_i)\) 是词频， \(thr\) 是概率阈值
                                    (在原文中, \(thr=10^{-5}\))。
                                    这个公式保留了频率的排名，但积极地对频率大于\(thr\)的单词进行二次采样。
                                    <br><br>
                                    有趣的是，这种启发式方法在实践中效果很好：它加速了学习，甚至显著提高了罕见词向量的准确性。
                                    <br><br>

                                    <h3><u>2. 与中心词的距离</u></h3>
                                    <img src="../resources/lectures/word_emb/research/w2v_position-min.png" alt=""
                                        style="margin-left:20px; float:right; width: 40%" />
                                    与之前关于基于计数的方法的<a href="#research_improve_count_based">练习
                                        on count-based methods</a>一样，我们可以为更接近中心的单词分配更高的权重。

                                    <br><br>
                                    乍一看，读者不会在原始 Word2Vec 实现中看到任何权重。 然而，在每一步中，它都会从 1 到 L
                                    对上下文窗口的大小进行采样。因此，更接近中心的单词比远离的单词更频繁地使用。
                                    在最初的工作中，这（可能）是为了提高效率（每个步骤的更新更少），但这也具有类似于分配权重的效果。


                                </details>

                            </div>
                        </div>

                        <!--##################################################-->


                        <!--##################################################-->
                        <div class="researchCard" id="thumbnail_research">
                            <div class="researchIntro" id="research_fasttext">

                                <div class="cardContent">

                                    <div class="research_title">
                                        使用有关子词的信息（“发明”FastText）
                                    </div>

                                    <hr color="#dedeca" style="margin:5px;">
                                    通常，我们有一个查找表，其中每个单词都分配有一个不同的向量。但在构造时，这些向量不知道它们包含的子词：它们拥有的所有信息都是它们从上下文中学到的。

                                </div>
                                <div>
                                    <!--<div class="research_tag">neural</div>-->

                                    <img src="../resources/lectures/word_emb/research/distinct_vectors-min.png" alt=""
                                        style="margin-top:20px;" class="center" />

                                </div>

                            </div>
                            <hr color="#dedeca" style="margin:5px">
                            <div class="cardContent">

                                <span class="research_question">?</span>
                                想象一下，当单词的词向量对它所包含的子词有一些了解后，可能会更好吗？<br>
                                <details>
                                    <summary class="research_summary">
                                        可能的答案
                                    </summary>
                                    <ul>
                                        <li><u>更好地理解形态学</u><br>
                                            为不同单词分配不同向量的做法忽略了形态学。然而，提供有关子词的信息可以让模型知道不同的单词可以是同一个词的不同形式。
                                        </li>
                                        <li><u>未知词的表示</u><br>
                                            通常，我们只能表示出现在词汇表中的那些词。 提供有关子词的信息可以帮助表示未知词。
                                        </li>
                                        <li><u>处理拼写错误</u><br>
                                            即使单词中的一个字符是错误的，这也会变成另一个单词，也因此是完全不同的词向量（甚至是未知单词）。
                                            有了关于子词的信息，拼错的词仍然与原始单词是相似的。
                                        </li>
                                    </ul>

                                </details>


                                <br>
                                <span class="research_question">?</span>
                                如何将有关子词的信息合并到词向量中？ 让我们假设训练方法还是一样的，例如，带有负采样的
                                Skip-Gram。
                                <details>
                                    <summary class="research_summary">
                                        现有方法之一（FastText）
                                        <center>
                                            <img src="../resources/lectures/word_emb/research/fasttext-min.png"
                                                style="max-width:75%; margin-bottom:15px;" />
                                        </center>

                                        一种可能的方法是从单词子词的向量合成该单词的词向量。 例如，流行的<a
                                            href="https://arxiv.org/pdf/1607.04606.pdf">FastText
                                        </a>操作如图所示。对于每个单词，该方法为每个单词添加特殊的开始和结束字符。然后，除了单词本身的词向量之外，FastText
                                        还使用字符级 n-gram 的向量（也在词汇表中）来补充单词的信息。一个词的表示是词及其子词的向量总和，如图所示。
                                        <br><br>
                                        请注意，这只改变了我们构造词向量的方式，但整个训练流程与标准 Word2Vec 中的相同。

                                </details>

                            </div>
                        </div>

                        <!--##################################################-->

                    </div>
                    <div class="research_circle" style="float:left;"></div>
                    <!--##################################################-->
                    <!--##################################################-->
                    <!--##################################################-->


                    <br><br>

                    <!--##################################################-->
                    <!--##################################################-->
                    <!--##################################################-->

                    <div class="research_circle" style="float:left;"></div>
                    <h2 style="margin-top:-10px; float: left; padding-left:10px; padding-right:10px; color:#786702">
                        语义变化</h2>
                    <div class="box_yellow_left">

                        <!--##################################################-->
                        <div class="researchCard" id="thumbnail_research">
                            <div class="researchIntro" id="research_meaning_shift">

                                <div class="cardContent">

                                    <div class="research_title">
                                        检测改变用法的单词
                                    </div>

                                    <hr color="#dedeca" style="margin:5px;">
                                    想象一下，你有来自不同来源的文本语料库，比如不同时间段，不同人群，不同区域等等。在数字人文和计算社会科学中，人们通常希望找到在跨语料库语义有变化的词。


                                </div>
                                <div>
                                    <!--<div class="research_tag">neural</div>-->

                                    <img src="../resources/lectures/word_emb/research/semantic_change-min.png" alt=""
                                        style="margin-top:20px;" class="center" />

                                </div>

                            </div>
                            <hr color="#dedeca" style="margin:5px">
                            <div class="cardContent">

                                <span class="research_question">?</span>
                                给定两个文本语料库，如何检测哪些单词的使用方式不同/具有不同的含义？别害怕，先考虑一些非常简单的方法！
                                <details>
                                    <summary class="research_summary">
                                        现有的一些尝试
                                    </summary>
                                    <h3><u>ACL 2020</u>: ACL 2020：训练嵌入，看看邻居</h3>
                                    <img src="../resources/lectures/word_emb/research/intersect_neighbors-min.png"
                                        alt="" style="margin-left:20px; float:right; width: 50%" />

                                    一种非常简单的方法是训练词嵌入（例如 Word2Vec）并查看最近的邻居。
                                    如果两个语料库中一个词的最近邻居不同，则可以视为该词改变了它的含义：因为词嵌入反映了其所处的上下文！
                                    <br><br>
                                    此方法在此 ACL 2020 论文中提出。 形式上，对于每个单词，作者在两个嵌入集中取 k
                                    个最近的邻居，并计算有多少邻居是相同的。共同邻居多的话表明语义没变，但如果共同邻居很少表明语义是不同的。
                                    <br><br>
                                    <p class="data_text">
                                        <font color="#888"><u>编者按</u>：请注意，虽然这种方法是最近才出现的，但它非常简单，并且比以前更复杂的想法效果更好。
                                            永远不要害怕尝试简单的事情——你会惊讶于它们的效果！</font>
                                    </p>


                                    <h3><u>以前流行的方法</u>：对齐两个嵌入空间</h3>
                                    <center>
                                        <img src="../resources/lectures/word_emb/research/emb_align-min.png"
                                            style="max-width:85%; margin-bottom:15px;" />
                                    </center>
                                    <a href="https://www.aclweb.org/anthology/P16-1141.pdf"
                                        target="_blank">以前流行的方法</a>是对齐两个嵌入空间并找到联合空间中不匹配的单词。
                                    形式化地，让 \(\color{#88a635}{W_1}\color{black}, \color{#547dbf}{W_2}\color{black} \in
                                    \mathbb{R}^{d\times |V|}\)
                                    代表词向量在不同语料库上训练得到的集合。 为了对齐学习的词向量，作者找到了旋转
                                    \(R = \arg \max\limits_{Q^TQ=I}\parallel \color{#547dbf}{W_2}\color{black}Q -
                                    \color{#88a635}{W_1}\color{black}\parallel_F\) - 这被称为正交
                                    Procrustes。使用这种旋转，我们可以对齐词向量空间并找到不匹配的词：这些词会随着语料库的变化而改变含义。

                                    <br><br>
                                    <p class="data_text">
                                        <font color="#888"><u>编者按</u>：请在作业中实现 Orthogonal
                                            Procrustes 方法来对齐俄罗斯语料和乌克兰语料分别训练出的词向量。 在<a
                                                href="https://github.com/yandexdataschool/nlp_course"
                                                target="_blank">课程仓库</a>中找到笔记本。</font>
                                    </p>

                                </details>


                            </div>
                        </div>


                    </div>
                    <div class="research_circle" style="float:left;"></div>



                </div>

                <br><br><br><br>


                <div id="related_papers">
                    <img height="40" src="../resources/lectures/ico/book_empty.png"
                        style="float:left; padding-right:10px; margin-top:-20px;" />
                    <h1 style="margin-left:10px; margin-right:20px; float: left; margin-top:-20px"><b>相关论文</b></h1>
                    <hr color="#facae9" style="height:5px">

                    <br><br>


                    <fieldset style="border: 1px solid #dec8d6;
    border-radius: 5px;">
                        <legend>
                            <p class="data_text"><strong>方法</strong></p>
                        </legend>
                        <ul class="data_text">
                            <li><u>概览速读</u>: 在简要总结中看一看关键结果—了解该领域的情况。

                            </li>
                            <li><u>更深入一些</u>: 对于你更感兴趣的主题，阅读包含图示和解释更长的摘要，浏览作者的推理步骤和关键观察结果。
                            </li>
                            <li><u>深度阅读</u>: 阅读你喜欢的论文。现在，当你理解了主要的想法，这会更容易！
                            </li>
                        </ul>
                    </fieldset>

                    <br><br>

                    <p class="data_text" style="font-size:24px;color:#7a3160">其中包括:</p>
                    <ul class="data_text" style="font-size:20px;color:#7a3160">
                        <li><a href="#papers_good_old_classics">经典论文(Good Old Classics)</a></li>
                        <li><a href="#papers_analyzing_geometry">分析几何(Analyzing Geometry) </a></li>
                        <li><a href="#papers_biases">词嵌入中的偏差(Biases in Word Embeddings) </a></li>
                        <li><a href="#papers_semantic_change_box">语义变化(Semantic Change) </a></li>
                        <li><a href="#papers_theory">拯救理论!(Theory to the Rescue!)</a> - 即将推出</li>
                        <li><a href="#papers_cross_lingual">跨语言嵌入(Cross-Lingual Embeddings)</a> - 即将推出</li>
                        <li>... 即将被更新</li>
                    </ul>


                    <br><br>


                    <!--##################################################-->
                    <!--##################################################-->
                    <!--##################################################-->

                    <div class="paper_circle" style="float:left;"></div>
                    <h2 style="margin-top:-10px; float: left; padding-left:10px; padding-right:10px; color:#7a3160">
                        Good Old Classics</h2>
                    <div class="box_pink_left" id="papers_good_old_classics">


                        <!--##################################################-->
                        <div class="paperCard" id="thumbnail_paper">
                            <div class="paperIntro">

                                <div class="cardContent">

                                    <div class="paper_title">
                                        <a href="https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf"
                                            target="_blank" class="links">
                                            Neural Word Embedding as Implicit Matrix Factorization
                                        </a>
                                    </div>

                                    <div class="paper_authors">
                                        Omer Levy, Yoav Goldberg
                                    </div>
                                    <hr color="#f2e4ee" style="margin:5px;">
                                    <p>从理论上讲，Word2Vec 与矩阵分解方法没有太大区别！ 带有负采样 (SGNS) 的 Skip-gram
                                        其实是在隐式逼近移位的逐点互信息 (PMI) 矩阵：
                                        \(PMI(\color{#88bd33}{w}\color{black}, \color{#888}{c}\color{black})-\log k\),
                                        其中 \(k\) 是负采样中的负样本数量。</p>
                                </div>

                                <div>
                                    <div class="conf_name">NeurIPS 2014</div>
                                    <a href="https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf"
                                        target="_blank">
                                        <img src="../resources/lectures/word_emb/papers/levy_goldberg_idea-min.png"
                                            alt="" style="margin-top:20px;" class="center" />
                                    </a>
                                </div>

                            </div>
                            <hr color="#f2e4ee" style="margin:5px">
                            <div class="cardContent">
                                <details>
                                    <summary style="margin-left:10px;">更多细节</summary>

                                    让我们回顾一下中心词 w 和上下文词 c 的损失函数：

                                    \[ J_{\color{#88bd33}{w}\color{black}, \color{#888}{c}}\color{black}(\theta)=
                                    \log\sigma(\color{#888}{u_{c}^T}\color{#88bd33}{v_{w}}\color{black}) +
                                    \sum\limits_{\color{#888}{ctx}\color{black}\in \{w_{i_1},\dots, w_{i_k}\}}
                                    \log(1-\sigma({\color{#888}{u_{ctx}^T}\color{#88bd33}{v_w}}\color{black})),
                                    \]
                                    其中 \(w_{i_1},\dots, w_{i_K}\) 是 在这一步取的\(k\)个负样本.
                                    <br><br>
                                    这是一步的loss，但是整个语料库的loss会是什么样子呢？ 当然，我们会多次遇到相同的词上下文对。

                                    <br><br>
                                    我们会遇到:
                                    <ul>
                                        <li>(<font color="#88bd33">w</font>, <font color="#888">c</font>) 中心词-上下文词对:
                                            \(N(\color{#88bd33}{w}\color{black}, \color{#888}{c}\color{black})\) times;
                                        </li>
                                        <li>
                                            <font color="#888">c</font> 作为负样本出现
                                            <font color="#88bd33">w</font>:
                                            \(
                                            \frac{kN(\color{#88bd33}{w}\color{black})N(\color{#888}{c}\color{black})}{N}\)
                                            次.<br>
                                            为什么：每次我们采样一个负样本，我们可以用概率\(\frac{N(\color{#888}{c}\color{black})}{N}\)选择c
                                            -
                                            c的频率。 乘以 N(w) 因为我们恰好遇到 w N(w) 次；
                                            乘以\(k\)，因为我们采样了\(k\)个负样本。</span>
                                        </li>
                                    </ul>

                                    因此，所有语料库的总损失为：
                                    \[ J(\theta)=\sum\limits_{\color{#88bd33}{w}\color{black}\in V,
                                    \color{#888}{c}\color{black} \in V}
                                    \left[N(\color{#88bd33}{w}\color{black}, \color{#888}{c}\color{black})\cdot
                                    \log\sigma(\color{#888}{u_{c}^T}\color{#88bd33}{v_{w}}\color{black}) +
                                    \frac{kN(\color{#88bd33}{w}\color{black})N(\color{#888}{c}\color{black})}{N}\cdot
                                    \log(1-\sigma(\color{#888}{u_{c}^T}\color{#88bd33}{v_{w}}\color{black}))\right].\]

                                    关于\(\color{#888}{u_{c}^T}\color{#88bd33}{v_{w}}\)的偏导数
                                    是（需要你自己检查一下）:
                                    <center>
                                        <img src="../resources/lectures/word_emb/papers/pmi_from_gradient-min.png"
                                            style="max-width:70%; margin-bottom:15px;" />
                                    </center>

                                    我们得到的是 Word2Vec (SGNS) 用一个最优值
                                    \(\color{#888}{u_{c}^T}\color{#88bd33}{v_{w}}\color{black} =
                                    PMI(\color{#88bd33}{w}\color{black}, \color{#888}{c}\color{black})-\log k\)优化了一些东西。

                                    这意味着它学习了\(\color{#888}{u_{c}}\) 和
                                    \(\color{#88bd33}{v_{w}}\)这样的向量，他们的点积等于 PMI 矩阵的元素向右移位\(\log k\)
                                    \(\Longrightarrow\)它隐式地学习分解这个移动的 PMI 矩阵。

                                    <br><br>
                                    这是对证明背后思想的一个相当直观的解释。 如需更正式的版本，请查看论文。 此外，作者使用这些结果直接分解移位的
                                    PMI 矩阵，并查看质量是否与 Word2Vec 相同。


                                </details>
                            </div>
                        </div>

                        <!--##################################################-->


                        <!--##################################################-->
                        <div class="paperCard" id="thumbnail_paper">
                            <div class="paperIntro">

                                <div class="cardContent">

                                    <div class="paper_title">
                                        <a href="https://www.aclweb.org/anthology/Q15-1016.pdf" target="_blank"
                                            class="links">
                                            Improving Distributional Similarity with Lessons Learned from Word
                                            Embeddings
                                        </a>

                                    </div>

                                    <div class="paper_authors">
                                        Omer Levy, Yoav Goldberg, Ido Dagan
                                    </div>
                                    <hr color="#f2e4ee" style="margin:5px;">
                                    <p>在某些时候，人们认为基于预测的词向量优于基于计数的词向量。 但事实并非如此：我们可以将
                                        word2vec 实现中的一些“技巧”应用到基于计数的模型中，并获得相同的结果。 此外，如果评估得当，GloVE
                                        比 Word2Vec 差。</p>

                                </div>

                                <div>
                                    <div class="conf_name">TACL 2015</div>
                                    <a href="https://www.aclweb.org/anthology/Q15-1016.pdf" target="_blank">
                                        <img src="../resources/lectures/word_emb/papers/levy_tacl15-min.png" alt=""
                                            style="margin-top:20px;" class="center" />
                                    </a>
                                </div>

                            </div>
                            <hr color="#f2e4ee" style="margin:5px">
                            <div class="cardContent">
                                <details>
                                    <summary style="margin-left:10px;">更多细节</summary>

                                    <p>
                                        该论文测试了许多超参数并进行了大量实验——我建议你研究一下。下面我将提供最重要的事情以供参考。</p>

                                    <h2>特征加权：“错误地”使用 SVD 更好</h2>

                                    <img width=100%
                                        src="../resources/lectures/word_emb/papers/eigenvalue_weighting-min.png" alt=""
                                        style="margin-bottom:15px;" />
                                    <p>通常，由 SVD 派生的词和上下文向量表示为
                                        \(V_d\Sigma_d\) 和 \(U_d\): 特征值矩阵仅包含在词向量中。 然而，对于单词相似度任务，这不是最佳结构。
                                        实验表明对称的变体更好：要么在单词和上下文向量中包括
                                        \(\sqrt{\Sigma_d}\)，或者在两者中都丢弃（看图）。</p>

                                    <img width=25% src="../resources/lectures/word_emb/papers/smooth_pmi-min.png" alt=""
                                        style="margin-left:20px; float:right;" />
                                    <h2>上下文分布平滑</h2>
                                    <p>正如我们在<a href="#choice_of_neg_examples">上面</a>中所讨论的，Word2Vec
                                        根据平滑的一元分布\(U^{3/4}\)对负样本进行采样,
                                        这样做是为了更频繁地挑选罕见词。</p>

                                    <p>在计算 PMI 时，我们可以做类似的事情：不使用真正的上下文分布，而是使用平滑的分布（看右图）。
                                        与 Word2Vec 一样, \(\alpha=0.75\).</p>

                                    <h2>Word2Vec 中心词向量和上下文词向量：尝试平均</h2>
                                    <p>回想一下，在训练后 GloVe 平均了中心词和上下文词的向量，而 Word2Vec 丢弃了上下文词向量。
                                        但是，有时 Word2Vec 也可以从平均中受益，可以尝试一下！</p>

                                    <h2>主要结果</h2>
                                    <ul>
                                        <li>通过调整超参数，基于预测的嵌入并不比基于计数的嵌入更好；</li>
                                        <li>通过一些简单的小技巧，Word2Vec (SGNS) 在每项任务上都能比 GloVe 更好。</li>

                                    </ul>
                                </details>
                            </div>
                        </div>

                        <!--##################################################-->

                    </div>
                    <div class="paper_circle" style="float:left;"></div>

                    <!--##################################################-->
                    <!--##################################################-->
                    <!--##################################################-->


                    <br><br>

                    <!--##################################################-->
                    <!--##################################################-->
                    <!--##################################################-->

                    <div class="paper_circle" style="float:left;"></div>
                    <h2 style="margin-top:-10px; float: left; padding-left:10px; padding-right:10px; color:#7a3160">
                        Analyzing Geometry</h2>
                    <div class="box_pink_left" id="papers_analyzing_geometry">

                        <div class="paperCard" id="thumbnail_paper">
                            <div class="paperIntro">

                                <div class="cardContent">

                                    <div class="paper_title">
                                        <a href="https://www.aclweb.org/anthology/D17-1308.pdf" target="_blank"
                                            class="links">
                                            The strange geometry of skip-gram with negative sampling
                                        </a>

                                    </div>

                                    <div class="paper_authors">
                                        David Mimno, Laure Thompson
                                    </div>
                                    <hr color="#f2e4ee" style="margin:5px;">
                                    负采样目标会影响词嵌入的几何形状：词向量 \(\color{#88bd33}{v_{w}}\)
                                    位于一个狭窄的圆锥体中，与上下文向量\(\color{#888}{u_{w}}\)截然相反。
                                    此外，与 GloVe 不同，Word2Vec 中的上下文词向量指向远离中心词向量。
                                </div>

                                <div>
                                    <div class="conf_name">EMNLP 2017</div>
                                    <a href="https://www.aclweb.org/anthology/D17-1308.pdf" target="_blank">
                                        <img width=90%
                                            src="../resources/lectures/word_emb/papers/emnlp17_strange_geometry-min.png"
                                            alt="" style="margin-top:20px;" class="center" />
                                    </a>
                                </div>

                            </div>
                            <hr color="#f2e4ee" style="margin:5px">
                            <div class="cardContent">
                                <details>
                                    <summary style="margin-left:10px;">更多细节</summary>

                                    <h2>词向量指向大致相同的方向</h2>

                                    <p>作者用所有向量的平均值评估不同频率词的向量点积。
                                        由于分布非常接近且点积为正，因此向量（大部分）指向平均向量的方向。</p>

                                    <center>
                                        <img src="../resources/lectures/word_emb/papers/strange_geometry_dot_prod_to_mean-min.png"
                                            style="max-width:90%; margin-bottom:15px;" />
                                    </center>

                                    <h2>上下文词向量远离词向量</h2>
                                    <p>在这里我们做同样的事情，但是取上下文词向量（平均值仍然是词向量）。 对于
                                        SGNS，上下文词向量与词向量均值的点积为负。
                                        这意味着上下文词向量指向远离词向量，使用它们是不合理的——我们把它们扔掉，只使用词向量。
                                        对于 GloVe，情况并非如此：上下文向量的行为方式与词向量相同。
                                    </p>
                                    <center>
                                        <img src="../resources/lectures/word_emb/papers/strange_geometry_dot_prod_to_ctx-min.png"
                                            style="max-width:100%; margin-bottom:15px;" />
                                    </center>

                                    这还不是全部 - 在论文中找到更多信息！

                                </details>
                            </div>
                        </div>

                        <div class="paperCard" id="thumbnail_paper">
                            <div class="paperIntro">

                                <div class="cardContent">

                                    <div class="paper_title">
                                        <a href="https://www.aclweb.org/anthology/P18-2036.pdf" target="_blank"
                                            class="links">
                                            Characterizing Departures from Linearity in Word Translation
                                        </a>

                                    </div>

                                    <div class="paper_authors">
                                        Ndapa Nakashole, Raphael Flauger
                                    </div>
                                    <hr color="#f2e4ee" style="margin:5px;">
                                    <p><a href="#analysis_cross_lingual">我们</a>
                                        了解到，我们可以（几乎）线性匹配不同语言的语义空间。 但是语言之间的“真正”映射真的是线性的吗？
                                        如果它是全局线性的，那么所有局部线性映射必须相似（与全局线性映射相似，因此彼此相似）。然而，他们并不是。
                                    </p>

                                </div>

                                <div>
                                    <div class="conf_name">ACL 2018</div>
                                    <a href="https://www.aclweb.org/anthology/P18-2036.pdf" target="_blank">
                                        <img src="../resources/lectures/word_emb/papers/acl18_departures_from_linearity-min.png"
                                            alt="" style="margin-top:20px;" class="center" />
                                    </a>
                                </div>

                            </div>
                            <hr color="#f2e4ee" style="margin:5px">
                            <div class="cardContent">
                                <details>
                                    <summary style="margin-left:10px;">更多细节</summary>
                                    <br>
                                    <p>如何检查语义空间之间的“真正”映射是否确实是线性的？ 主要思想如图所示。</p>
                                    <center>
                                        <img src="../resources/lectures/word_emb/papers/global_linearity_idea-min.png"
                                            style="max-width:100%; margin-bottom:15px;" />
                                    </center>

                                    <h2>局部跨语言映射不相似</h2>
                                    <p>为了检查局部映射是否相似，作者</p>
                                    <ul>
                                        <li>对于选择的词，取它们的邻域：一组具有至少一些余弦相似度的词；</li>
                                        <li>对于每个邻域，找到另一种语言的相应单词集；</li>
                                        <li>建立局部交叉线性映射；</li>
                                        <li>评估这些映射的相似程度：对于两个映射 \(M_1\) 和
                                            \(M_2\) （例如，对于单词的邻域 \(w_1\) 和 \(w_2\)),
                                            计算矩阵的向量化版本之间的余弦相似度
                                            \(M_1\) 和 \(M_2\).
                                        </li>
                                    </ul>

                                    <h2>对于遥远的词，它们的局部跨语言映射是不同的</h2>
                                    <p>作者发现，</p>
                                    <ul>
                                        <li>不同区域的局部映射可能非常不同。 因此，语义空间之间的“真正”跨语言映射不是线性的；
                                        </li>
                                        <li>对于更远的单词，局部跨语言映射更加不同。</li>
                                    </ul>


                                </details>
                            </div>
                        </div>

                        <div class="paperCard" id="thumbnail_paper">
                            <div class="paperIntro">

                                <div class="cardContent">

                                    <div class="paper_title">
                                        <a href="https://openreview.net/pdf?id=HkuGJ3kCb" target="_blank" class="links">
                                            All-But-The-Top: Simple and Effective Post-Processing for Word
                                            Representations
                                        </a>
                                    </div>

                                    <div class="paper_authors">
                                        Jiaqi Mu, Pramod Viswanath
                                    </div>
                                    <hr color="#f2e4ee" style="margin:5px;">
                                    <p class="data_text" style="color:#888"><u>编者按</u>：这是分析如何提高质量的一个例子！
                                    </p>
                                    <p> 作者注意到（i）嵌入具有非零均值和（ii）早期奇异值比其他值大得多。
                                        当作者消除这些属性时，他们在内在和外在评估方面都得到了很大的改进。</p>
                                </div>

                                <div>
                                    <div class="conf_name">ICLR 2018</div>
                                    <a href="https://openreview.net/pdf?id=HkuGJ3kCb" target="_blank">
                                        <img src="../resources/lectures/word_emb/papers/all_but_the_top-min.png" alt=""
                                            style="margin-top:30px;" class="center" />
                                    </a>
                                </div>

                            </div>
                            <hr color="#f2e4ee" style="margin:5px">
                            <div class="cardContent">
                                <details>
                                    <summary style="margin-left:10px;">更多细节</summary>
                                    <br>

                                    <h2>Step 1: Analyze</h2>
                                    <img src="../resources/lectures/word_emb/papers/all_but_top_svd_decay-min.png"
                                        alt="" style="width:35%;float:right;margin-left:20px;margin-top:-20px;" />
                                    <p>对于不同的词嵌入模型和语言，作者发现向量</p>
                                    <ul>
                                        <li>
                                            <b>有一个很大的非零均值</b>
                                        </li>
                                        <li>
                                            <b>不是各向同性的</b><br>
                                            看图：如果 \(\sigma_i\) 是奇异值，那么对于小的 \(i\), 它们几乎呈指数衰减，其余部分大致保持不变。
                                        </li>
                                    </ul>


                                    <img src="../resources/lectures/word_emb/papers/all_but_top_first_two_pca-min.png"
                                        alt="" style="width:65%;float:right;margin-left:20px;" />
                                    <p>此外，作者注意到顶级 PCA 成分编码的内容与语义无关：例如词频。</p>

                                    <br>
                                    <p>这些属性似乎与语义无关，即在词嵌入中对我们很重要的东西。 如果我们消除这些影响呢？
                                        会更好吗？</p>

                                    <h2>第 2 步：使用观察来提高质量</h2>
                                    <p>为了消除发现的属性，作者</p>
                                    <ul>
                                        <li>
                                            <b>从词向量中减去它们的平均值</b>
                                        </li>
                                        <li>
                                            <b>消除顶级 PCA 成分</b><br>
                                            让 \(u_1, \dots, u_d\) 作为单词向量的PCA 成分 \(\{v_w, w\in V\}\).
                                            然后向量更新如下：
                                            \[v_w \longleftarrow v_w - \sum\limits_{i=1}^d(u_i^Tv_w)u_i.\]
                                        </li>
                                    </ul>

                                    <p>结果：各种任务的巨大改进，包括内在（相似性和类比）和外在（监督分类）。</p>

                                </details>
                            </div>
                        </div>

                    </div>
                    <div class="paper_circle" style="float:left;"></div>

                    <!--##################################################-->
                    <!--##################################################-->
                    <!--##################################################-->


                    <br><br>


                    <!--##################################################-->
                    <!--##################################################-->
                    <!--##################################################-->

                    <div class="paper_circle" style="float:left;"></div>
                    <h2 style="margin-top:-10px; float: left; padding-left:10px; padding-right:10px; color:#7a3160">
                        Biases in Word Embeddings</h2>
                    <div class="box_pink_left" id="papers_biases">

                        <div class="paperCard" id="thumbnail_paper">
                            <div class="paperIntro">

                                <div class="cardContent">

                                    <div class="paper_title">
                                        <a href="https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf"
                                            target="_blank" class="links">
                                            Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word
                                            Embeddings
                                        </a>

                                    </div>

                                    <div class="paper_authors">
                                        Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam Kalai
                                    </div>
                                    <hr color="#f2e4ee" style="margin:5px;">
                                    <p>词嵌入是有偏见的。 例如，虽然他们的类比推理可能是可取的，例如
                                        “男人对女人就像国王对王后”，“男人对女人就像医生对护士”，这是一种不合意的联想。</p>
                                </div>

                                <div>
                                    <div class="conf_name">NeurIPS 2016</div>
                                    <a href="https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf"
                                        target="_blank">
                                        <img src="../resources/lectures/word_emb/papers/gender_bias-min.png" alt=""
                                            style="margin-top:20px;" class="center" />
                                    </a>
                                </div>

                            </div>
                            <hr color="#f2e4ee" style="margin:5px">
                            <div class="cardContent">
                                <details>
                                    <summary style="margin-left:10px;">更多细节</summary>
                                    <br>
                                    <h2>问题：嵌入有偏差</h2>
                                    <p>作者注意到词嵌入是有偏见的：它们编码了不想要的性别关联。
                                        为了找到这样的例子，他们采用种子对（例如，（a，b）=（he，she））并找到具有相同关联的词对：在相同方向上彼此不同，并且彼此相对接近
                                        其他。 形式上，他们选择得分高的配对：
                                    </p>

                                    <img width=60% src="../resources/lectures/word_emb/papers/he_she_criteria-min.png"
                                        alt="" style="margin-top:20px;" class="center" />
                                    <p>看看下面的结果——肯定有些对是有偏差的！</p>
                                    <img width=80% src="../resources/lectures/word_emb/papers/he_she_results-min.png"
                                        alt="" style="margin-top:10px;margin-bottom:15px;" class="center" />
                                    <p>这意味着，例如，不仅“男人对女人就像国王对王后”，这是一种理想的行为，而且“男人对女人就像医生对护士”，这是一种理想的行为。
                                        不希望的关联。
                                    </p>


                                    <img width=30%
                                        src="../resources/lectures/word_emb/papers/he_she_projections-min.png" alt=""
                                        style="margin-left:20px;float:right;" />
                                    <h3>性别陈规定型职业</h3>
                                    <p>为了找到最具性别刻板印象的职业，作者将职业投影到他-她性别方向上。 结果显示在右侧。</p>
                                    <p>我们可以看到，例如，家庭主妇、护士、图书管理员、造型师大多与女性相关，而船长、魔术师、建筑师、战士与男性的相关性更强。
                                    </p>

                                    <h2>去偏词嵌入</h2>
                                    <p> 在原始论文中，作者还提出了几种启发式方法来消除词嵌入的偏差——将不想要的关联作为后处理步骤移除。
                                        由于最近在去偏方面做了很多工作，有关此特定方法的更多详细信息，请参阅原始论文。
                                        有关更新的方法，请查看下一篇论文。
                                    </p>

                                </details>
                            </div>
                        </div>

                        <div class="paperCard" id="thumbnail_paper">
                            <div class="paperIntro" id="paper_null_it_out">

                                <div class="cardContent">

                                    <div class="paper_title">
                                        <a href="https://www.aclweb.org/anthology/2020.acl-main.647.pdf" target="_blank"
                                            class="links">
                                            Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection
                                        </a>

                                    </div>

                                    <div class="paper_authors">
                                        Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, Yoav Goldberg
                                    </div>
                                    <hr color="#f2e4ee" style="margin:5px;">
                                    <p>对 debias 词嵌入的迭代零空间投影：</p>
                                    <ul>
                                        <li>训练一个线性分类器\(W\)来预测嵌入的属性（例如，性别）,</li>
                                        <li>在 \(W\) 的零空间 (\(x \rightarrow Px\), \(W(Px)=0\) 上线性投影嵌入 -
                                            删除用于预测的信息；
                                        </li>
                                        <li>重复直到分类器无法预测任何东西。</li>
                                    </ul>

                                </div>

                                <div>
                                    <div class="conf_name">ACL 2020</div>
                                    <a href="https://www.aclweb.org/anthology/2020.acl-main.647.pdf" target="_blank">
                                        <img src="../resources/lectures/word_emb/papers/null_it_out-min.png" alt=""
                                            style="margin-top:20px;" class="center" />
                                    </a>
                                </div>

                            </div>
                            <hr color="#f2e4ee" style="margin:5px">
                            <div class="cardContent">
                                <details>
                                    <summary style="margin-left:10px;">更多细节</summary>
                                    <br>

                                    <h2>想法：删除线性分类器使用的信息</h2>
                                    <p>我们必须删除有关某些所需属性（例如性别）的信息，但不能损害嵌入的其他属性。
                                        作者提出了一个非常简单的想法：训练一个线性分类器从嵌入中预测这个属性，然后删除这个分类器使用的信息。</p>

                                    <img width=90% src="../resources/lectures/word_emb/papers/null_it_out_idea-min.png"
                                        alt="" style="margin-top:20px;" class="center" />

                                    <p>如果分类器是线性的，那么移除部分可以很容易地完成：通过投影到分类器的决策边界上。
                                        这种投影是删除有关属性的线性信息的危害最小的方法：它尽可能少地损害嵌入之间的距离。</p>

                                    <p>该方法是迭代的：您必须重复此过程（训练分类器并投影到新的决策边界），直到分类器无法预测任何有意义的东西。
                                        当分类器无法预测属性时，我们知道所有信息都已被删除。</p>


                                    <h2>结果：一切都好</h2>
                                    <img width=25% src="../resources/lectures/word_emb/papers/null_it_out_steps-min.png"
                                        alt="" style="margin-left:20px;float:right;" />

                                    <p>在原始论文中，您会发现实验表明该方法：</p>
                                    <ul>
                                        <li>确实消除了偏见<br>（看右边的插图：在算法的 0、3、18、35 次迭代中，最具性别偏见的词的
                                            GloVe 向量的 t-SNE 投影），
                                        </li>
                                        <li>不损害嵌入质量<br>
                                            （例如，在去偏之前和之后查看最近的邻居：见下文）。
                                        </li>
                                    </ul>

                                    <img width=90%
                                        src="../resources/lectures/word_emb/papers/null_it_out_before_after-min.png"
                                        alt="" style="margin-top:10px;margin-bottom:10px;" class="center" />

                                    <p>有关更正式的内容和更多结果和示例，请查看原始论文。</p>


                                </details>
                            </div>
                        </div>

                    </div>
                    <div class="paper_circle" style="float:left;"></div>



                    <br><br>



                    <div class="paper_circle" style="float:left;"></div>
                    <h2 style="margin-top:-10px; float: left; padding-left:10px; padding-right:10px; color:#7a3160">
                        语义变化</h2>
                    <div class="box_pink_left" id="papers_semantic_change_box">
                        <br><br>
                        <p style="margin-top:-20px;">
                            想象一下，您有来自不同来源的文本语料库：时间段、人口、地理区域等。在这一部分中，任务是找到在这些语料库中使用不同的单词。</p>


                        <!--##################################################-->
                        <div id="paper_semantic_change_mapping_alert" style="display:block;">
                            <div class="paperCard" id="thumbnail_paper">

                                <div class="paperIntro">

                                    <div class="cardContent">

                                        <div class="paper_title">
                                            <a href="#paper_semantic_change_mapping_alert" class="links">
                                                Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change
                                            </a>

                                        </div>

                                        <div class="paper_authors">
                                            William L. Hamilton, Jure Leskovec, Dan Jurafsky
                                        </div>
                                        <hr color="#f2e4ee" style="margin:5px;">

                                        <p class="data_text" style="color:#888"><u>编者按</u>：这篇论文用于研究思考部分。
                                            在这里，我向您隐藏了链接和内容-最好去那里思考。 但如果你确实想要，你可以在这里了解这篇论文。
                                        </p>
                                    </div>

                                    <div>
                                        <div class="conf_name">ACL 2016</div>
                                        <p style="font-size:30px;text-align:center;margin-top:30px;">Spoiler alert!</p>

                                        <center>
                                            <img width=70% src="../resources/lectures/ico/dont_want_to_think.png" alt=""
                                                style="margin-top:-10px;" class="center" />
                                        </center>

                                        <center>
                                            <img class="showMePaper" width=70% onclick="openPaper_semChange1()"
                                                style="cursor:pointer;"
                                                src="../resources/lectures/ico/show_me_paper_lightgrey.png" alt=""
                                                style="margin-top:10px;" class="center" />
                                        </center>


                                    </div>

                                </div>

                            </div>
                        </div><!-- end of <div id="paper_semantic_change_neighbors_alert"> -->

                        <div id="paper_semantic_change_mapping" style="display:none;">
                            <div class="paperCard" id="thumbnail_paper">

                                <div class="paperIntro">

                                    <div class="cardContent">

                                        <div class="paper_title">
                                            <a href="https://www.aclweb.org/anthology/P16-1141.pdf" target="_blank"
                                                class="links">
                                                Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change
                                            </a>

                                        </div>

                                        <div class="paper_authors">
                                            William L. Hamilton, Jure Leskovec, Dan Jurafsky
                                        </div>
                                        <hr color="#f2e4ee" style="margin:5px;">
                                        <p>为了找出哪些词在两个文本语料库中的使用是不同的: </p>
                                        <ul>
                                            <li>使用每个语料库训练嵌入，</li>
                                            <li>将两个嵌入空间线性映射到一起，</li>
                                            <li>向量不匹配的词是改变了其含义的词。</li>
                                        </ul>

                                    </div>

                                    <div>
                                        <div class="conf_name">ACL 2016</div>
                                        <a href="https://www.aclweb.org/anthology/P16-1141.pdf" target="_blank">
                                            <img src="../resources/lectures/word_emb/papers/broadcast_semantic_change-min.png"
                                                alt="" style="margin-top:10px;" class="center" />
                                        </a>
                                    </div>

                                </div>


                                <hr color="#f2e4ee" style="margin:5px">
                                <div class="cardContent">

                                    <a onclick="closePaper_semChange1()" class="data_text"
                                        style="background-color:#f3f3f3;padding:5px;color:black;float:right;">
                                        I've changed my mind - hide ⤒</a>

                                    <details>
                                        <summary style="margin-left:10px;">More details</summary>
                                        <br>

                                        <h2><u>Idea</u>: Align Two Embedding Sets, Find Words That Do Not Match</h2>
                                        <center>
                                            <img src="../resources/lectures/word_emb/research/emb_align-min.png"
                                                style="max-width:85%; margin-bottom:15px;" />
                                        </center>
                                        <p>The main idea here is to align two embeddings sets and to find words
                                            whose embeddings do not match well. Formally, let
                                            \(\color{#88a635}{W_1}\color{black},
                                            \color{#547dbf}{W_2}\color{black} \in
                                            \mathbb{R}^{d\times |V|}\)
                                            be embedding sets trained on different corpora.
                                            To align the learned embeddings, the authors find the rotation
                                            \[R = \arg \max\limits_{Q^TQ=I}\parallel \color{#547dbf}{W_2}\color{black}Q
                                            -
                                            \color{#88a635}{W_1}\color{black}\parallel_F.\]
                                            This
                                            is called Orthogonal Procrustes. Using this rotation, we can align embedding
                                            sets
                                            and find words that do not match well: these are the words that change
                                            meaning with the corpora.</p>

                                        <p>Once the embedding sets are aligned, we can evaluate the <font face="arial">
                                                semantic
                                                displacement</font>.
                                            Let \(\color{#88a635}{v_w^1}\) and \(\color{#547dbf}{v_w^2}\) be embedding
                                            of a
                                            word \(w\) in the two aligned spaces,
                                            then the semantic displacement
                                            is
                                            \(1- \cos (\color{#88a635}{v_w^1}\color{black},
                                            \color{#547dbf}{v_w^2}\color{black}).\)
                                            Intuitively, this measures how well embeddings of the same word <font
                                                face="arial">"match"</font>
                                            in the aligned semantic spaces.</p>

                                        <h2><u>Experiments</u></h2>
                                        <h3>TL;DR: SGNS Embeddings are Better than PPMI and SVD(PPMI)</h3>

                                        <p>The authors looked at historical texts for different time periods and tried
                                            to
                                            apply the method
                                            on top of different embeddings: PPMI matrix, SVD(PPMI) and Word2Vec (SGNS).
                                            Below are examples of
                                            the top words found for each of the embedding methods.</p>
                                        <center>
                                            <img src="../resources/lectures/word_emb/papers/historical_top10-min.png"
                                                style="max-width:100%; margin-bottom:15px;" />
                                        </center>

                                        <ul>
                                            <li><strong>bold</strong> - real semantic shifts (validated by examining
                                                literature)<br>
                                                <span class="data_text" style="color:#888;">E.g.,
                                                    <strong>headed</strong> shifted from primarily referring to the "top
                                                    of a body/entity"
                                                    to referring to "a direction of travel."</span>
                                            </li>
                                            <li><u>underlined</u> - borderline cases (largely due
                                                to global genre/discourse shifts)<br>
                                                <span class="data_text" style="color:#888;">E.g., <strong>male</strong>
                                                    has not changed in meaning, but its usage in discussions
                                                    of “gender equality” is relatively new.</span>
                                            </li>
                                            <li>unmarked - clear corpus artifacts<br>
                                                <span class="data_text" style="color:#888;">E.g., special, cover, and
                                                    romance are artifacts from the covers of fiction books occasionally
                                                    including
                                                    advertisements etc.</span>
                                            </li>
                                        </ul>
                                        <p>Looks like results obtained for SGNS embeddings are better.
                                            In <a href="https://www.aclweb.org/anthology/P16-1141.pdf">the original
                                                paper</a>, different kinds of evaluation were used to confirm this more
                                            formally.</p>

                                        <p>From <a href="#papers_semantic_change_box">the next paper</a>, you will learn
                                            how
                                            to detect semantic change more easily.</p>

                                        <h2>Note: The Alignment Idea is Used for Different Tasks</h2>
                                        <p>Note that the idea to linearly map different semantic spaces was also used
                                            for
                                            other tasks.
                                            For example, <a href="#analysis_cross_lingual">earlier in the lecture</a> we
                                            aligned semantic spaces for different languages to build vocabulary.</p>

                                        <p> For more advanced methods
                                            for building cross-lingual embeddings, look <a
                                                href="#papers_cross_lingual">here
                                                in the Related Papers</a>.</p>

                                    </details>
                                </div>

                            </div>
                        </div> <!-- end of <div id="paper_semantic_change_neighbors"> -->

                        <!--##################################################-->
                        <script>
                            function openPaper_semChange1() {
                                document.getElementById("paper_semantic_change_mapping").style.display = "block";
                                document.getElementById("paper_semantic_change_mapping_alert").style.display = "none";
                            }

                            function closePaper_semChange1() {
                                document.getElementById("paper_semantic_change_mapping").style.display = "none";
                                document.getElementById("paper_semantic_change_mapping_alert").style.display = "block";
                            }
                        </script>


                        <!--##################################################-->
                        <div id="paper_semantic_change_neighbors_alert" style="display:block;">
                            <div class="paperCard" id="thumbnail_paper">

                                <div class="paperIntro">

                                    <div class="cardContent">

                                        <div class="paper_title">
                                            <a href="#paper_semantic_change_neighbors_alert" class="links">
                                                Simple, Interpretable and Stable Method for Detecting Words with Usage
                                                Change across Corpora
                                            </a>

                                        </div>

                                        <div class="paper_authors">
                                            Hila Gonen, Ganesh Jawahar, Djamé Seddah, Yoav Goldberg
                                        </div>
                                        <hr color="#f2e4ee" style="margin:5px;">

                                        <p class="data_text" style="color:#888"><u>编者按</u>：这篇论文用于研究思考部分。
                                            在这里，我向您隐藏了链接和内容-最好去那里思考。 但如果你确实想要，你可以在这里了解这篇论文。
                                        </p>
                                    </div>

                                    <div>
                                        <div class="conf_name">ACL 2020</div>
                                        <p style="font-size:30px;text-align:center;margin-top:30px;">Spoiler alert!</p>

                                        <center>
                                            <img width=70% src="../resources/lectures/ico/dont_want_to_think.png" alt=""
                                                style="margin-top:-10px;" class="center" />
                                        </center>

                                        <center>
                                            <img class="showMePaper" width=70% onclick="openPaper_semChange2()"
                                                style="cursor:pointer;"
                                                src="../resources/lectures/ico/show_me_paper_lightgrey.png" alt=""
                                                style="margin-top:10px;" class="center" />
                                        </center>


                                    </div>

                                </div>

                            </div>
                        </div><!-- end of <div id="paper_semantic_change_neighbors_alert"> -->

                        <div id="paper_semantic_change_neighbors" style="display:none;">
                            <div class="paperCard" id="thumbnail_paper">

                                <div class="paperIntro">

                                    <div class="cardContent">

                                        <div class="paper_title">
                                            <a href="https://www.aclweb.org/anthology/2020.acl-main.51.pdf"
                                                target="_blank" class="links">
                                                Simple, Interpretable and Stable Method for Detecting Words with Usage
                                                Change across Corpora
                                            </a>

                                        </div>

                                        <div class="paper_authors">
                                            Hila Gonen, Ganesh Jawahar, Djamé Seddah, Yoav Goldberg
                                        </div>
                                        <hr color="#f2e4ee" style="margin:5px;">
                                        <p>To find which words are used differently in two text corpora: </p>
                                        <ul>
                                            <li>train embeddings using each of the corpora,</li>
                                            <li>for each word, find closest neighbors in the two embedding spaces;</li>
                                            <li>the neighbors differ a lot → the words are used differently.</li>
                                        </ul>

                                    </div>

                                    <div>
                                        <div class="conf_name">ACL 2020</div>
                                        <a href="https://www.aclweb.org/anthology/2020.acl-main.51.pdf" target="_blank">
                                            <img src="../resources/lectures/word_emb/papers/semantic_change_2-min.png"
                                                alt="" style="margin-top:30px;" class="center" />
                                        </a>
                                    </div>

                                </div>


                                <hr color="#f2e4ee" style="margin:5px">
                                <div class="cardContent">

                                    <a onclick="closePaper_semChange2()" class="data_text"
                                        style="background-color:#f3f3f3;padding:5px;color:black;float:right;">
                                        I've changed my mind - hide ⤒</a>

                                    <details>
                                        <summary style="margin-left:10px;">More details</summary>
                                        <br>

                                        <h2><u>Idea</u>: Train Embeddings, Look at the Neighbors</h2>
                                        <img src="../resources/lectures/word_emb/research/intersect_neighbors-min.png"
                                            alt="" style="margin-left:20px; float:right; width: 60%" />

                                        <p>A very simple approach
                                            is to train embeddings (e.g., Word2Vec) and look at the closest neighbors.
                                            If a word's closest neighbors are different for the two corpora, the word
                                            changed
                                            its meaning: remember that word embeddings reflect contexts they saw!</p>

                                        <p>Formally, for each word \(w\) the authors take k nearest neighbors
                                            in the two embeddings sets: \(NN_1^k(w)\) and \(NN_2^k(w)\). Then they count
                                            how
                                            many neighbors are the same
                                            and define the <b>change score</b> as follows:
                                            \[score^k(w) = -|NN_1^k(w)\cap NN_2^k(w)|\]
                                            A large intersection
                                            means that the meaning is not different (the score will be low), small
                                            intersection - meaning is different
                                            (such words will receive a high score).</p>

                                        <h2>The Method is Interpretable</h2>

                                        <p>By design, the method is interpretable: it explains its decisions (i.e., why
                                            the
                                            word is used differently)
                                            by showing the closest neighbors of the word in the two embedding spaces.
                                            These
                                            neighbors reflect
                                            the word meanings in the two corpora. Look at the examples of found words
                                            along
                                            with the closest neighbors.
                                        </p>
                                        <center>
                                            <img src="../resources/lectures/word_emb/papers/semantic_nn_examples-min.png"
                                                alt="" style="width: 100%" />
                                        </center>
                                        <!-- <p>Let us recall that in the previous approach, we also looked at the closest neighbors of the words
                    found by the method to understand how they changed their meaning. However, in that case,
                        this wasn't an explanation of the method decision. On the contrary, here
                        closest neighbors provide an explicit explanation of the method.</p>-->

                                        <br>
                                        <h2>Other Good Things</h2>
                                        <p>Compared to the alignment-based methods (e.g., <a
                                                href="#papers_semantic_change_box">the previous paper</a>),
                                            this approach: </p>
                                        <ul>
                                            <li>is more stable,</li>
                                            <li>requires less tuning and word filtering.</li>
                                        </ul>

                                        <p>For more details, look at the paper.</p>


                                        <p class="data_text">
                                            <font color="#888"><u>Lena:</u> Note that while the approach is
                                                recent,
                                                it is extremely simple and works better than previous more complicated
                                                ideas.
                                                Never be afraid to try simple things - you'll be surprised how often
                                                they work!</font>
                                        </p>


                                    </details>
                                </div>

                            </div>
                        </div> <!-- end of <div id="paper_semantic_change_neighbors"> -->

                        <!--##################################################-->
                        <script>
                            function openPaper_semChange2() {
                                document.getElementById("paper_semantic_change_neighbors").style.display = "block";
                                document.getElementById("paper_semantic_change_neighbors_alert").style.display = "none";
                            }

                            function closePaper_semChange2() {
                                document.getElementById("paper_semantic_change_neighbors").style.display = "none";
                                document.getElementById("paper_semantic_change_neighbors_alert").style.display = "block";
                            }
                        </script>

                    </div>
                    <div class="paper_circle" style="float:left;"></div>

                    <!--##################################################-->
                    <!--##################################################-->
                    <!--##################################################-->


                    <br><br>


                    <!--##################################################-->
                    <!--##################################################-->
                    <!--##################################################-->

                    <div class="paper_circle" style="float:left;"></div>
                    <h2 style="margin-top:-10px; float: left; padding-left:10px; padding-right:10px; color:#7a3160">
                        拯救理论！</h2>
                    <div class="box_pink_left" id="papers_theory">

                        <br><br>

                        <div style="border: 0px solid #ccc;border-radius:15px;margin: 10px; padding: 4px;
 background-color: #f5f5f5;margin-top:-20px;">

                            <div style="display: grid; grid-template-columns: 75% 25%; margin:10px;">
                                <div>
                                    <div style="margin-top:20px;">
                                        <font style="font-size:25px;">即将推出:</font>

                                        <ul>
                                            <li>
                                                <a href="https://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding.pdf"
                                                    target="_blank" class="links">
                                                    On the Dimensionality of Word Embedding
                                                </a>
                                            </li>
                                            <li><a href="http://proceedings.mlr.press/v97/allen19a/allen19a.pdf"
                                                    target="_blank" class="links">
                                                    Analogies Explained: Towards Understanding Word Embeddings
                                                </a></li>
                                        </ul>
                                    </div>
                                </div>
                                <div>
                                    <center>
                                        <img src="../resources/lectures/main/preview/pusheen_draws_on_white-min.png"
                                            style="width:80%; padding-top:20px; padding-bottom:20px;border-radius:50%">
                                    </center>
                                </div>
                            </div>

                        </div>

                    </div>
                    <div class="paper_circle" style="float:left;"></div>

                    <br><br>


                    <div class="paper_circle" style="float:left;"></div>
                    <h2 style="margin-top:-10px; float: left; padding-left:10px; padding-right:10px; color:#7a3160">
                        跨语言词嵌入</h2>
                    <div class="box_pink_left" id="papers_cross_lingual">

                        <br><br>

                        <div style="border: 0px solid #ccc;border-radius:15px;margin: 10px; padding: 4px;
 background-color: #f5f5f5;margin-top:-20px;">

                            <div style="display: grid; grid-template-columns: 75% 25%; margin:10px;">
                                <div>
                                    <div style="margin-top:20px;">
                                        <font style="font-size:25px;">即将推出:</font>

                                        <ul>
                                            <li><a href="https://arxiv.org/abs/1710.04087" target="_blank"
                                                    class="links">
                                                    Word Translation Without Parallel Data
                                                </a></li>
                                            <li>... to be updated</li>

                                        </ul>
                                    </div>
                                </div>
                                <div>
                                    <center>
                                        <img src="../resources/lectures/main/preview/pusheen_reads_on_white-min.png"
                                            style="width:80%; padding-top:20px; padding-bottom:20px;border-radius:50%">
                                    </center>
                                </div>
                            </div>

                        </div>


                    </div>
                    <div class="paper_circle" style="float:left;"></div>

                    <br><br>


                </div>


                <br><br>

                <div id="have_fun">
                    <img height="40" src="../resources/lectures/ico/fun_empty.png"
                        style="float:left; padding-right:10px; margin-top:-20px;" />
                    <h1 style="margin-left:10px; margin-right:20px; float: left; margin-top:-20px">Have Fun!</h1>
                    <hr color="#c8edfa" style="height:5px">
                    <br><br>

                    <fieldset style="border: 1px solid #008cbf;
    border-radius: 5px;font-size:18px;">
                        <legend>
                            <p class="data_text" style="font-size:24px;">
                                <font color="#008cbf"><strong>语义空间冲浪者</strong></font>
                            </p>
                        </legend>
                        <p class="data_text">
                            通常，我们希望词嵌入能够像人类一样进行推理。 但是让我们尝试相反的情况：您将尝试将其视为词嵌入。
                        </p>
                        <p class="data_text">您将看到类比示例，例如 国王 - 男人 + 女人 = ?，以及几个可能的答案。 任务是猜测词嵌入的想法。
                        </p>

                        <p class="data_text">完成任务（10 个示例）并获得语义空间冲浪者证书！</p>

                        <p class="data_text" style="font-size:14px;">
                            <font color="#888">词嵌入：我们使用了来自 gensim-data 的 glove-twitter-100。
                        </p>

                        <p class="data_text" style="font-size:14px;">
                            非常感谢 Just Heuristic 对技术问题的帮助！ 只是启发式 - 只是有趣！ </p>
                    </fieldset>

                    <br><br>


                    <center>
                        <div class="quiz_window" id="semantic_space_surfer" style="width: 80%;">
                            <div class="cardContent">
                                <p class="prompt_text"></p>
                                <hr class="hr_in_question" color="#e9f0f2" style="margin:5px;">
                                <div class="answer_block">
                                    <div class="answer_container" style="margin-right: 50px;"></div>
                                    <div class="next" style="transform: translateX(-50px);">
                                        <div class="next_button"></div>
                                        <p class="next_text">next</p>
                                    </div>
                                </div>
                                <p class="comment_text"></p>

                                <div class="result_block" style="display: none;">
                                    <p class="result_header">Semantic Space Surfer: Level 0</p>
                                    <p class="result_course_mention"><strong>NLP course <font color="#92bf32">| For
                                                YOU</font></strong>: Official Certificate</p>
                                    <hr color="#e9f0f2" style="margin:5px;">
                                    <div class="quiz_result"></div>
                                </div>
                                <progress class="progressbar" max="100" value="0"></progress>
                            </div>
                        </div>
                    </center>

                    <script>
                        var all_questions = [
                            // %%%%%%%%%%%%%%  1 - 10 %%%%%%%%%%%%%%%%%%%%
                            ['coder - brain + money = ?', // question
                                'broker', // correct answer
                                ['banker', 'designer', 'shopper', 'freelance', 'promoter', 'ecommerce', 'makemoney', 'consultant', 'billing', 'retail'], // wrong options
                                ['You were close!', 'Well, almost', '', 'if only...', 'Close!', 'Naaah', 'This would be too simple', 'Nay', 'Almost there!', 'Naaah']], // comments for wrong options

                            ['italy - pizza + burger = ?',
                                'denmark',
                                ['america', 'germany', 'spain', 'sweden', 'ireland', 'switzerland', 'england', 'russia'],
                                ['You are too human', '', '', '', '', '', '', '']],

                            ['cat - good + bad = ?',
                                'dog',
                                ['pet', 'kitten', 'puppy', 'rat', 'hamster', 'pig', 'monkey', 'horse'],
                                ['', 'Why on earth do you think so?!', 'Why do you think so?!', 'I agree! Maybe we are too human', '', '', '', '']],

                            ['cat - scary + cute = ?',
                                'kitty',
                                ['puppy', 'dog', 'pet', 'cutie', 'baby', 'bear', 'panda'],
                                ['Not cute enough', 'Naaa', 'Too vague', 'Too cute', '', 'If a bear is not scary for you, what is?!', 'Too cute']],

                            ['mom - tired + sleep = ?',
                                'sister',
                                ['dad', 'grandma', 'friend', 'husband', 'brother', 'daughter', 'wife', 'aunt', 'cousin'],
                                ['', '', '', '', '', '', '', '', '']],

                            ['student - study + sleep = ?',
                                'work',
                                ['school', 'home', 'friend', 'right', 'wake', 'drunk'],
                                ['', '', '', '', '', 'Easy, pal!']],

                            ['education - money + knowledge = ?',
                                'leadership',
                                ['intelligence', 'ethics', 'literacy', 'science', 'philosophy', 'journalism', 'principles', 'educational', 'learning', 'understanding', 'academic', 'diversity'],
                                ['', '', '', '', '', '', '', '', '', '', '', '']],

                            ['education - knowledge + money = ?',
                                'pay',
                                ['business', 'cash', 'tax', 'budget', 'bills', 'work'],
                                ['Almost!', 'Nah', 'Nay', 'Looks reasonable though', '', 'Too human']],

                            ['table - mouse + cat = ?',
                                'kitchen',
                                ['room', 'floor', 'front', 'bath', 'bathroom', 'chair', 'pub', 'house', 'place'],
                                ['', '', '', '', '', '', '', '', '']],

                            ['table - cat + mouse = ?',
                                'keyboard',
                                ['desk', 'corner', 'console', 'box', 'screen', 'cabinet', 'poker', 'front', 'hardware'],
                                ['', '', '', '', '', '', '', '', '']],

                            // %%%%%%%%%%%%%%  11 - 20 %%%%%%%%%%%%%%%%%%%%

                            ['hedgehog - night + day = ?',
                                'bunny',
                                ['puppy', 'penguin', 'dalek', 'squirrel', 'sleep'],
                                ['', '', '', '', '']],

                            ['vampire - blood + food = ?',
                                'movie',
                                ['sushi', 'vegetarian', 'foodie'],
                                ['', '', '']],

                            ['dinner - night + day = ?',
                                'lunch',
                                ['breakfast', 'meal', 'food', 'cooking', 'cake', 'brunch'],
                                ['Too early', 'Too vague', 'Too vague', 'Days for cooking, nights for eating?..', '', 'You were close!']],

                            ['dinner - wine + tea = ?',
                                'lunch',
                                ['breakfast', 'cake', 'brunch', 'burger', 'eat', 'pizza', 'snack'],
                                ['', '', '', '', '', '', '']],

                            ['breakfast - coffee + tea = ?',
                                'dinner',
                                ['lunch', 'meal', 'cake', 'brunch', 'dessert', 'snack', 'pancakes'],
                                ['', '', '', '', '', '', '']],

                            ['wine - alcohol + vitamin = ?',
                                'cherry',
                                ['olive', 'coffee', 'almond', 'citrus', 'coconut', 'lime', 'cream', 'lemon'],
                                ['', '', '', '', '', '', '', '']],

                            ['morning - coffee + milk = ?',
                                'boo',
                                ['day', 'baby', 'honey', 'night', 'sunday', 'today'],
                                ['Nay', 'Too human', '', '', "That's what I thought, but no", 'If this is how your "today" looks like - sorry :(']],

                            ['angel - good + evil = ?',
                                'devil',
                                ['lucifer', 'demon', 'shadow', 'alien', 'tarzan', 'judas'],
                                ['', '', '', '', '', '']],

                            ['angel - food + drink = ?',
                                'eva',
                                ['karina', 'abel', 'stella', 'amber', 'diana', 'luna', 'melody', 'lucy', 'miranda', 'dani', 'anna'],
                                ['', '', '', '', '', '', '', '', '', '', '']],

                            ['twitter - word + photo = ?',
                                'instagram',
                                ['facebook', 'tumblr', 'flickr', 'fb', 'page', 'pic', 'blog'],
                                ['', '', '', '', '', '', '']],

                            // %%%%%%%%%%%%%%  21 - 30 %%%%%%%%%%%%%%%%%%%%

                            ['instagram - photo + word = ?',
                                'twitter',
                                ['ask.fm', 'bullshit', 'whatsapp', 'facebook', 'app', 'imessage', 'account'],
                                ['', '', '', '', '', '', '']],

                            ['facebook - word + photo = ?',
                                'twitter',
                                ['ask.fm', 'whatsapp', 'internet', 'tweets', 'app', 'msn', 'fb', 'whats'],
                                ['', '', '', '', '', '', '', '']],

                            ['person - no + yes = ?',
                                'kind',
                                ['guy', 'exactly', 'nice', 'definitely'],
                                ['', '', '', '']],

                            ['girl - skirt + pants = ?',
                                'boy',
                                ['guy', 'like', 'swear', 'dude', 'friend', 'kid', 'you', 'right', 'mom'],
                                ['', '', '', '', '', '', '', '', '']],

                            ['gato - cat + dog = ?',
                                'cachorro',
                                ['gordo', 'loco', 'burro', 'pesado', 'amigo', 'chico'],
                                ['', '', '', '', '', '']],

                            ['yoga - meditation + exercise = ?',
                                'workout',
                                ['gym', 'jogging', 'fitness', 'strength', 'sport', 'guru', 'abs', 'vacation'],
                                ['', '', '', '', '', '', '', '']],

                            ['dinner - meat + vegetable = ?',
                                'brunch',
                                ['lunch', 'soup', 'salad', 'breakfast', 'lasagna', 'thanksgiving', 'dessert', 'veggie', 'avocado'],
                                ['', '', '', '', '', '', '', '', '']],

                            ['lasagna - meat + vegetable = ?',
                                'quinoa',
                                ['hummus', 'avocado', 'zucchini', 'butternut', 'chickpea', 'lentil', 'salad', 'carbonara', 'spinach', 'casserole', 'soup', 'veggie'],
                                ['', '', '', '', '', '', '', '', '', '', '', '']],

                            ['language - good + bad = ?',
                                'grammar',
                                ['spelling', 'translation', 'understand', 'speak', 'stupid', 'accent'],
                                ['', '', '', '', '', '']],

                            ['reality - day + night = ?',
                                'dreams',
                                ['show', 'television', 'behind', 'into', 'happens', 'hollywood', 'true', 'life'],
                                ['', '', '', '', '', '', '', '']],

                            // %%%%%%%%%%%%%%  31 - 40 %%%%%%%%%%%%%%%%%%%%

                            ['tomato - red + green = ?',
                                'avocado',
                                ['salad', 'vegetable', 'garlic', 'spinach', 'parsley', 'mushroom', 'soup', 'coriander', 'veg', 'basil', 'cucumber'],
                                ['', '', '', '', '', '', '', '', '', '', '']],

                            ['salad - vegetable + meat = ?',
                                'chicken',
                                ['cheese', 'steak', 'sandwich', 'fried', 'sauce', 'burger', 'bread', 'lunch', 'fish'],
                                ['', '', '', '', '', '', '', '', '']],

                            ['duolingo - language + programming = ?',
                                'coursera',
                                ['node.js', 'phonegap', 'xcode', 'netbeans', 'twitterrific', 'arcgis', 'powershell', 'realplayer'],
                                ['', '', '', '', '', '', '', '']],

                            ['skype - video + word = ?',
                                'imessage',
                                ['facetime', 'wapp', 'msn', 'ping', 'whapp', 'viber'],
                                ['', '', '', '', '', '']],

                            ['skype - video + voice = ?',
                                'imessage',
                                ['facetime', 'chat', 'kik', 'phone', 'viber', 'msn', 'tel', 'conversation'],
                                ['', '', '', '', '', '', '', '']],

                            ['piano - large + small = ?',
                                'guitar',
                                ['violin', 'dance', 'drums', 'singing', 'ukulele', 'play', 'karate'],
                                ['', '', '', '', '', '', '']],

                            ['night - dark + light = ?',
                                'morning',
                                ['day', 'tonight', 'today', 'sunday', 'good', 'afternoon', 'saturday', 'evening'],
                                ['', '', '', '', '', '', '', '']],

                            ['lion - large + small = ?',
                                'cat',
                                ['simba', 'dog', 'monkey', 'tiger', 'fly'],
                                ['', '', '', '', '']],

                            ['bear - large + small = ?',
                                'monkey',
                                ['baby', 'dog', 'cat', 'puppy', 'boy', 'daddy', 'bunny'],
                                ['', '', '', '', '', '', '']],

                            ['dad - large + small = ?',
                                'mom',
                                ['sister', 'brother', 'friend', 'grandma', 'kid', 'grandpa', 'dude'],
                                ['', '', '', '', '', '', '']],


                            // %%%%%%%%%%%%%%  41 - 50 %%%%%%%%%%%%%%%%%%%%

                            ['sandwich - cold + hot = ?',
                                'burger',
                                ['steak', 'sushi', 'cheese', 'pizza', 'chicken', 'bacon', 'potato', 'sauce', 'cake', 'nachos'],
                                ['', '', '', '', '', '', '', '', '', '']],

                            ['water - drink + eat = ?',
                                'fish',
                                ['food', 'ground', 'meat', 'chicken', 'salt', 'rice', 'sand', 'grass'],
                                ['', '', '', '', '', '', '', '']],

                            ['coder - code + music = ?',
                                'musician',
                                ['scientist', 'singer', 'guitarist', 'composer', 'songwriter', 'hip-hop', 'rocker', 'practitioner', 'pianist', 'frontman', 'vocalist'],
                                ['', '', '', '', '', '', '', '', '', '', '']],

                            ['musician - music + science = ?',
                                'researcher',
                                ['psychologist', 'scholar', 'psychology', 'engineering', 'writer', 'historian', 'journalist'],
                                ['', '', '', '', '', '', '']],

                            ['guitarist - guitar + voice = ?',
                                'singer',
                                ['vocalist', 'bassist', 'rapper', 'tribute', 'drummer', 'goldie', 'producer', 'chris', 'comedian'],
                                ['', '', '', '', '', '', '', '', '']],

                            ['hand - write + walk = ?',
                                'feet',
                                ['front', 'side', 'floor', 'lift', 'leg', 'door', 'ground', 'down'],
                                ['', '', '', '', '', '', '', '']],

                            ['boat - water + sky = ?',
                                'plane',
                                ['cruise', 'sunset', 'yacht', 'moon', 'harbour', 'jet'],
                                ['', '', '', '', '', '']],

                            ['bird - sky + water = ?',
                                'fish',
                                ['flappy', 'fruit', 'goat', 'cow', 'turtle', 'salt'],
                                ['', '', '', '', '', '']],

                            ['life - laziness + inspiration = ?',
                                'story',
                                ['amazing', 'world', 'great', 'beautiful', 'heart', 'made', 'quotes', 'dream'],
                                ['', '', '', '', '', '', '', '']],

                            ['computer - penguin + apple = ?',
                                'microsoft',
                                ['windows', 'samsung', 'tablet', 'iphone', 'google', 'ipad', 'laptop', 'apps', 'blackberry', 'mobile', 'nokia'],
                                ['', '', '', '', '', '', '', '', '', '', '']],


                            // %%%%%%%%%%%%%%  51 - 60 %%%%%%%%%%%%%%%%%%%%

                            ['sandwich - cold + hot = ?',
                                'burger',
                                ['steak', 'sushi', 'cheese', 'pizza', 'chicken', 'bacon', 'potato', 'sauce', 'cake', 'nachos'],
                                ['', '', '', '', '', '', '', '', '', '']],

                            ['water - drink + eat = ?',
                                'fish',
                                ['food', 'ground', 'meat', 'chicken', 'salt', 'rice', 'sand', 'grass'],
                                ['', '', '', '', '', '', '', '']],

                            ['coder - code + music = ?',
                                'musician',
                                ['scientist', 'singer', 'guitarist', 'composer', 'songwriter', 'hip-hop', 'rocker', 'practitioner', 'pianist', 'frontman', 'vocalist'],
                                ['', '', '', '', '', '', '', '', '', '', '']],

                            ['musician - music + science = ?',
                                'researcher',
                                ['psychologist', 'scholar', 'psychology', 'engineering', 'writer', 'historian', 'journalist'],
                                ['', '', '', '', '', '', '']],

                            ['guitarist - guitar + voice = ?',
                                'singer',
                                ['vocalist', 'bassist', 'rapper', 'tribute', 'drummer', 'goldie', 'producer', 'chris', 'comedian'],
                                ['', '', '', '', '', '', '', '', '']],

                            ['hand - write + walk = ?',
                                'feet',
                                ['front', 'side', 'floor', 'lift', 'leg', 'door', 'ground', 'down'],
                                ['', '', '', '', '', '', '', '']],

                            ['boat - water + sky = ?',
                                'plane',
                                ['cruise', 'sunset', 'yacht', 'moon', 'harbour', 'jet'],
                                ['', '', '', '', '', '']],

                            ['bird - sky + water = ?',
                                'fish',
                                ['flappy', 'fruit', 'goat', 'cow', 'turtle', 'salt'],
                                ['', '', '', '', '', '']],

                            ['life - laziness + inspiration = ?',
                                'story',
                                ['amazing', 'world', 'great', 'beautiful', 'heart', 'made', 'quotes', 'dream'],
                                ['', '', '', '', '', '', '', '']],

                            ['computer - penguin + apple = ?',
                                'microsoft',
                                ['windows', 'samsung', 'tablet', 'iphone', 'google', 'ipad', 'laptop', 'apps', 'blackberry', 'mobile', 'nokia'],
                                ['', '', '', '', '', '', '', '', '', '', '']],


                            // %%%%%%%%%%%%%%  61 - 70 %%%%%%%%%%%%%%%%%%%%

                            ['wolverine - night + day = ?',
                                'thor',
                                ['x-men', 'ironman', 'superman', 'hulk', 'spiderman', 'deadpool', 'xmen', 'gandalf', 'jackman'],
                                ['', '', '', '', '', '', '', '', '']],

                            ['catwoman - cat + man = ?',
                                'batman',
                                ['knight', 'tarantino', 'thor', 'stark', 'spiderman', 'robocop'],
                                ['', '', '', '', '', '']],

                            ['superman - super + woman = ?',
                                'devil',
                                ['father', 'wife', 'werewolf', 'priest', 'superhero', 'witch', 'spiderman'],
                                ['', '', '', '', '', '', '']],

                            ['superhero - super + hero = ?',
                                'villain',
                                ['wonderwoman', 'werewolf', 'character', 'spiderman', 'godfather', 'ghostbusters', 'marvel', 'batman', 'magician'],
                                ['', '', '', '', '', '', '', '', '']],

                            ['werewolf - wolf + hero = ?',
                                'superhero',
                                ['villain', 'musician', 'footballer', 'ultraman', 'fighter', 'superstar', 'photographer'],
                                ['', '', '', '', '', '', '']],

                            ['ultraman - ultra + man = ?',
                                'traitor',
                                ['paman', 'politician', 'tsubasa', 'aang', 'husband', 'kunde', 'hans', 'rvp', 'captain', 'persie'],
                                ['', '', '', '', '', '', '', '', '', '']],

                            ['frankenstein - psycho + hero = ?',
                                'ghostbusters',
                                ['raj', 'thor', 'gandalf', 'avengers', 'villain', 'marvel', 'knight', 'merlin'],
                                ['', '', '', '', '', '', '', '']],

                            ['thor - mighty + weak = ?',
                                'hulk',
                                ['batman', 'spiderman', 'dude', 'loki', 'robocop'],
                                ['', '', '', '', '']],

                            ['ironman - men + iron = ?',
                                'thor',
                                ['spiderman', 'avengers', 'superman', 'looper', 'wolverine', 'marvel', 'hobbit', 'robocop', 'godzilla'],
                                ['', '', '', '', '', '', '', '', '']],

                            ['spiderman - spider + cats = ?',
                                'batman',
                                ['superman', 'animals', 'dogs', 'minions', 'wolverine'],
                                ['', '', '', '', '']],


                            // %%%%%%%%%%%%%%  71 - 80 %%%%%%%%%%%%%%%%%%%%

                            ['thor - loki + hobbit = ?',
                                'trailer',
                                ['batman', 'django', 'spiderman', 'thrones', 'prometheus', 'knight', 'robocop'],
                                ['', '', '', '', '', '', '']],

                            ['loki - bad + good = ?',
                                'hiddleston',
                                ['thor', 'payne', 'harry', 'klaus', 'boris'],
                                ['', '', '', '', '']],

                            ['superman - man + animals = ?',
                                'turtles',
                                ['cats', 'dinosaurs', 'aliens', 'elephants', 'transformers', 'robots'],
                                ['', '', '', '', '', '']],

                            ['aliens - sky + earth = ?',
                                'humans',
                                ['dinosaurs', 'robots', 'creatures', 'species', 'animals', 'monsters', 'superheroes', 'zombies', 'directionators'],
                                ['', '', '', '', '', '', '', '', '']],

                            ['zombies - dead + alive = ?',
                                'aliens',
                                ['robots', 'krewella', 'dinosaurs', 'dreams', 'killers', 'humans', 'devil', 'miracles'],
                                ['', '', '', '', '', '', '', '']],

                            ['scientist - human + alien = ?',
                                'robot',
                                ['nostradamus', 'starship', 'zombie', 'gorilla', 'rogue', 'fireflies', 'astronaut'],
                                ['', '', '', '', '', '', '']],

                            ['sherlock - harry + hermione = ?',
                                'moriarty',
                                ['mycroft', 'himym', 'hitchcock', 'frankenstein', 'conan', 'moffat'],
                                ['', '', '', '', '', '']],

                            ['davidtennant - tennant + tardis = ?',
                                'drwho',
                                ['mattsmith', 'dalek', 'sherlock', 'batman', 'moffat'],
                                ['', '', '', '', '']],

                            ['potter - radcliffe + davidtennant = ?',
                                'drwho',
                                ['sherlock', 'batman', 'hermione'],
                                ['', '', '']],

                            ['dalek - tardis + hogwarts = ?',
                                'muggle',
                                ['westeros', 'hamlet', 'sherlock', 'batman', 'hermione'],
                                ['', '', '', '', '']],

                            // %%%%%%%%%%%%%%  81 - 90 %%%%%%%%%%%%%%%%%%%%

                            ['burger - round + square = ?',
                                'kfc',
                                ['mcdonald', 'sushi', 'hotdog', 'bakery', 'bagel', 'buffet', 'steak', 'pizza'],
                                ['', '', '', '', '', '', '', '']],

                            ['cadbury - round + square = ?',
                                'toblerone',
                                ['wafer', 'magnum', 'bambu', 'cokelat', 'oreo', 'almond', 'gingerbread'],
                                ['', '', '', '', '', '', '']],

                            ['gingerbread - square + round = ?',
                                'pumpkin',
                                ['cookie', 'whip', 'pudding', 'cake', 'baking', 'homemade', 'crack', 'vanilla'],
                                ['', '', '', '', '', '', '', '']],

                            ['shortbread - square + round = ?',
                                'biscuits',
                                ['entwined', 'pudding', 'neigh', 'nookie', 'truffles', 'ice-cream', 'nilla', 'bezzie'],
                                ['', '', '', '', '', '', '', '']],

                            ['student - hungry + wealthy = ?',
                                'corporate',
                                ['grads', 'academic', 'equity', 'colleges', 'education', 'universities', 'funded', 'faculty', 'scholar'],
                                ['', '', '', '', '', '', '', '', '']],

                            ['sausage - small + big = ?',
                                'bacon',
                                ['cheese', 'toast', 'steak', 'sandwich', 'chicken', 'ham', 'pancakes', 'roast', 'egg'],
                                ['', '', '', '', '', '', '', '', '']],

                            ['egg - boiled + fried = ?',
                                'chicken',
                                ['cheese', 'sandwich', 'cake', 'fish', 'salad', 'bread', 'rice', 'steak', 'sausage', 'burger', 'soup'],
                                ['', '', '', '', '', '', '', '', '', '', '']],

                            ['spock - potter + hermione = ?',
                                'uhura',
                                ['randal', 'nicholson', 'blaire', 'capuano', 'reeves', 'regan'],
                                ['', '', '', '', '', '']],

                            ['spock - startrek + starwars = ?',
                                'anakin',
                                ['jedi', 'gandalf', 'skywalker', 'darth', 'lego', 'sheldon', 'superman', 'percy', 'chuck', 'sylvester', 'bruce', 'bob'],
                                ['', '', '', '', '', '', '', '', '', '', '', '']],

                            ['jedi - starwars + startrek = ?',
                                'spock',
                                ['terminator', 'aryan', 'warlock', 'spidey', 'anakin', 'mech', 'fenrir', 'magneto', 'cyclops'],
                                ['', '', '', '', '', '', '', '', '']],

                            // %%%%%%%%%%%%%%  91 - 100 %%%%%%%%%%%%%%%%%%%%

                            ['mathematician - career + life = ?',
                                'philosopher',
                                ['proverb', 'cynic', 'glutton', 'misunderstood', 'socrates', 'pothead', 'psychopath', 'colorblind', 'drunkard'],
                                ['', '', '', '', '', '', '', '', '']],

                            ['sheldon - male + female = ?',
                                'leonard',
                                ['carl', 'cooper', 'bruce', 'doug', 'lenny', 'marshall', 'steven'],
                                ['', '', '', '', '', '', '']],

                            ['thebigbangtheory - sheldon + potter = ?',
                                'harrypotter',
                                ['twilight', 'saga', 'hungergames', 'dragonball', 'narnia', 'starwars'],
                                ['', '', '', '', '', '']],

                            ['thebigbangtheory - penny + hermione = ?',
                                'theamazingspiderman',
                                ['mycroft', 'twoandahalfmen', 'grint', 'tautou', 'westwick'],
                                ['', '', '', '', '']],

                            ['c++ - language + drink = ?',
                                'cider',
                                ['smirnoff', 'lemonade', 'alc', 'linux', 'redbull', 'wisk', 'juice', 'lime', 'fanta'],
                                ['', '', '', '', '', '', '', '', '']],

                            ['vodka - drink + eat = ?',
                                'nutella',
                                ['banana', 'bacon', 'chocolate', 'butter', 'cheese', 'bananas', 'pasta', 'sandwich', 'pizza', 'waffles', 'oreos', 'milk'],
                                ['', '', '', '', '', '', '', '', '', '', '', '']],

                            ['bacon - eat + drink = ?',
                                'vodka',
                                ['beer', 'drinks', 'whiskey', 'coke', 'soda', 'ketchup', 'coffee', 'lemonade', 'tequila', 'wine', 'pepsi', 'whisky'],
                                ['', '', '', '', '', '', '', '', '', '', '', '']],

                            ['javascript - language + drink = ?',
                                'smirnoff',
                                ['cider', 'coffee', 'soda', 'coffe', 'juice', 'heineken', 'vodka', 'redbull'],
                                ['', '', '', '', '', '', '', '']],

                            ['c++ - language + animal = ?',
                                'linux',
                                ['java', 'developer', 'acnl', 'perl', 'html'],
                                ['', '', '', '', '']],

                            ['java - language + animal = ?',
                                'zombie',
                                ['minecraft', 'iguana', 'safari', 'monster', 'resident', 'manhattan', 'green', 'farm', 'orangutan'],
                                ['', '', '', '', '', '', '', '', '']],


                            // %%%%%%%%%%%%%%  101 - 110 %%%%%%%%%%%%%%%%%%%%

                            ['copywriting - copy + writing = ?',
                                'screenwriting',
                                ['ghostwriting', 'blogging', 'journaling', 'freelancing'],
                                ['', '', '', '']],

                            ['jazz - piano + fork = ?',
                                'lobster',
                                ['longhorn', 'salt', 'grill', 'baltimore', 'sausage', 'shrimp', 'meat', 'olive', 'barbeque'],
                                ['', '', '', '', '', '', '', '', '']],

                            ['writer - shame + fame = ?',
                                'producer',
                                ['publisher', 'author', 'blogger', 'indie', 'artist', 'editor', 'entrepreneur', 'creator'],
                                ['', '', '', '', '', '', '', '']],

                            ['research - code + novelty = ?',
                                'stimulating',
                                ['translational', 'speculative', 'educational'],
                                ['', '', '']],

                            ['lobster - sea + earth = ?',
                                'crab',
                                ['fish', 'bread', 'squirrel', 'cheese', 'bagel'],
                                ['', '', '', '', '']],

                            ['spock - calm + emotional = ?',
                                'uhura',
                                ['kirk', 'gandalf', 'frodo'],
                                ['', '', '']],

                            ['research - science + art = ?',
                                'design',
                                ['photography', 'graphic', 'illustration', 'portrait', 'blog', 'project'],
                                ['', '', '', '', '', '']],

                            ['sport - medal + health = ?',
                                'lifestyle',
                                ['fitness', 'nutrition', 'wellness', 'community'],
                                ['', '', '', '']],

                            ['startup - money + science = ?',
                                'innovation',
                                ['storytelling', 'entrepreneurship', 'technology', 'institute', 'enterprise', 'computing'],
                                ['', '', '', '', '', '']],

                            ['innovation - novelty + money = ?',
                                'business',
                                ['marketing', 'startup', 'education'],
                                ['', '', '']],

                            // %%%%%%%%%%%%%%  111 - 120 %%%%%%%%%%%%%%%%%%%%

                            ['sun - hot + cold = ?',
                                'rain',
                                ['storm', 'moon', 'earth', 'darkness', 'winter'],
                                ['', '', '', '', '']],

                            ['human - sentient + furry = ?',
                                'dog',
                                ['cat', 'bear', 'child', 'monkey', 'big', 'kid', 'boy'],
                                ['', '', '', '', '', '', '']],

                            ['hamburger - america + italy = ?',
                                'lasagne',
                                ['tiramisu', 'toastie', 'spaghetti', 'croissant', 'kebab', 'baguette', 'hotdog', 'puding', 'gnocchi'],
                                ['', '', '', '', '', '', '', '', '']],

                            ['nutella - chocolate + banana = ?',
                                'yogurt',
                                ['pasta', 'ketchup', 'butter', 'sandwich', 'waffle', 'oreos', 'avocado'],
                                ['', '', '', '', '', '', '']],

                            ['wizardry - fantasy + science = ?',
                                'mathematics',
                                ['humanities', 'ethics', 'linguistics', 'theology', 'anthropology', 'coding', 'physics'],
                                ['', '', '', '', '', '', '']],

                            ['humanities - human + language = ?',
                                'linguistics',
                                ['literature', 'sociology', 'economics', 'mathematics', 'geography', 'geometry', 'civics', 'physics'],
                                ['', '', '', '', '', '', '', '']],

                            ['calculus - formula + shape = ?',
                                'geometry',
                                ['physics', 'chem', 'biology', 'geography', 'sociology', 'midterm', 'messed'],
                                ['', '', '', '', '', '', '']],

                            ['math - thinking + politics = ?',
                                'economics',
                                ['science', 'physics', 'ethics', 'sociology'],
                                ['', '', '', '']],

                            ['bar - alcohol + coffee = ?',
                                'cafe',
                                ['restaurant', 'bistro', 'grill', 'garden'],
                                ['', '', '', '']],

                            ['box - square + round = ?',
                                'pack',
                                ['bag', 'table', 'luggage', 'fox'],
                                ['', '', '', '']],

                            // %%%%%%%%%%%%%%  121 - 130 %%%%%%%%%%%%%%%%%%%%

                            ['dream - good + scare = ?',
                                'nightmare',
                                ['haunted', 'ghost', 'creepy', 'insidious', 'strange', 'sinister'],
                                ['', '', '', '', '', '']],

                            ['dream - night + day = ?',
                                'world',
                                ['life', 'wish', 'love', 'fact', 'future', 'everything', 'thing'],
                                ['', '', '', '', '', '', '']],

                            ['shampoo - hair + body = ?',
                                'spray',
                                ['lotion', 'conditioner', 'axe', 'mineral', 'deodorant', 'dispenser', 'paracetamol'],
                                ['', '', '', '', '', '', '']],

                            ['food - body + soul = ?',
                                'family',
                                ['music', 'heaven', 'bread'],
                                ['', '', '']],

                            ['water - body + soul = ?',
                                'fire',
                                ['joy', 'ocean', 'sky'],
                                ['', '', '']],

                            ['waffle - waff + soul = ?',
                                'cream',
                                ['ice', 'life', 'taste', 'sound', 'heart', 'fruit'],
                                ['', '', '', '', '', '']],

                            ['horror - scary + funny = ?',
                                'comedy',
                                ['story', 'epic', 'hilarious', 'drama', 'book'],
                                ['', '', '', '', '']],

                            ['shark - scary + funny = ?',
                                'fish',
                                ['dolphin', 'turtle', 'tiger', 'goat', 'whale', 'duck', 'squid'],
                                ['', '', '', '', '', '', '']],

                            ['green - blue + red = ?',
                                'orange',
                                ['brown', 'purple', 'yellow', 'pink'],
                                ['', '', '', '']],

                            ['vodka - russia + scotland = ?',
                                'whisky',
                                ['tequila', 'pint', 'champagne', 'licor', 'martini'],
                                ['', '', '', '', '']],


                            // %%%%%%%%%%%%%%  131 - 140 %%%%%%%%%%%%%%%%%%%%

                            ['vodka - russia + america = ?',
                                'tequila',
                                ['whisky', 'pint', 'champagne', 'licor', 'martini', 'coca', 'nutella'],
                                ['', '', '', '', '', '', '']],

                            ['cookie - cook + wait = ?',
                                'christmas',
                                ['bread', 'sandwich', 'soup', 'yogurt', 'easter'],
                                ['', '', '', '', '']],

                            ['starbucks - bucks + soul = ?',
                                'cafe',
                                ['heaven', 'sushi', 'chocolate', 'music', 'angel'],
                                ['', '', '', '', '']],

                            ['starbucks - star + food = ?',
                                'mcdonalds',
                                ['milkshake', 'donuts', 'cookies'],
                                ['', '', '']],

                            ['man - brain + emotion = ?',
                                'true',
                                ['lie', 'respect', 'drama'],
                                ['', '', '']],

                            ['head - hair + brain = ?',
                                'mind',
                                ['inside', 'mouth', 'thoughts', 'deep', 'hear', 'stomach'],
                                ['', '', '', '', '', '']],

                            ['elena - russia + scotland = ?',
                                'bonnie',
                                ['lucy', 'phoebe', 'blaine', 'katherine', 'stana'],
                                ['', '', '', '', '']],

                            ['bear - russia + scotland = ?',
                                'puppy',
                                ['forest', 'cat', 'dog', 'bunny', 'kitty', 'fluffy'],
                                ['', '', '', '', '', '']],

                            ['analogy - valid + crazy = ?',
                                'funny',
                                ['reminds', 'weird', 'badass', 'insane', 'unreal', 'dude', 'strange'],
                                ['', '', '', '', '', '', '']],

                            ['remind - re + mind = ?',
                                'knowing',
                                ['reason', 'doubt', 'either', 'forget', 'anything'],
                                ['', '', '', '', '']],

                            // %%%%%%%%%%%%%%  141 - 150 %%%%%%%%%%%%%%%%%%%%


                        ];
                        var result_messages = [
                            "<img src='../resources/lectures/word_emb/fun/level_0_2.png' width=30% style='margin:30px;'><br><p style='text-align:center;'>Remove your cat from the screen and try again</p>",
                            "<img src='../resources/lectures/word_emb/fun/level_0_2.png' width=30% style='margin:30px;'><br><p style='text-align:center;'>Remove your cat from the screen and try again</p>",
                            "<img src='../resources/lectures/word_emb/fun/level_0_2.png' width=30% style='margin:30px;'><br><p style='text-align:center;'>Remove your cat from the screen and try again</p>",
                            "<img src='../resources/lectures/word_emb/fun/level_3.png' width=30% style='margin:30px;'><br><p style='text-align:center;'>You've tried the water, but you need more training</p>",
                            "<img src='../resources/lectures/word_emb/fun/level_4.png' width=30% style='margin:30px;'><br><p style='margin-left:60px;margin-right:60px;text-align:center;'>It’s hard to keep balance, but you’ve made the first steps - well done!</p>",
                            "<img src='../resources/lectures/word_emb/fun/level_5.png' width=25% style='margin:30px;'><br><p style='margin-left:60px;margin-right:60px;text-align:center;'>Good junior level!</p>",
                            "<img src='../resources/lectures/word_emb/fun/level_6.png' width=40% style='margin:30px;'><br><p style='margin-left:60px;margin-right:60px;text-align:center;'>More than a half - you’ve graduated from a surfing school!</p>",
                            "<img src='../resources/lectures/word_emb/fun/level_7.png' width=30% style='margin:30px;'><br><p style='text-align:center;'>You do it like a pro!</p>",
                            "<img src='../resources/lectures/word_emb/fun/level_8.png' width=40% style='margin:30px;'><br><p style='text-align:center;'>You were born a surfer!</p>",
                            "<img src='../resources/lectures/word_emb/fun/level_9.png' width=45% style='margin:30px;'><br><p style='margin-left:60px;margin-right:60px;text-align:center;'>Your surfing is so good, that it is hard to keep up with you!</p>",
                            "<img src='../resources/lectures/word_emb/fun/level_10.png' width=40% style='margin:30px;'><br><p style='text-align:center;'>Your surfing is true magic! Think about Hogwarts.</p>",
                        ] // n-th element will be displayed if user got n questions right

                        var result_certificates = ["pic.twitter.com/mPNDkKiBXf",
                            "pic.twitter.com/n18Nlc8iLX",
                            "pic.twitter.com/brRQX4tMCx",
                            "pic.twitter.com/hf4rTNPjAm",
                            "pic.twitter.com/ngxRPQoI4w",
                            "pic.twitter.com/pVTlQTsj2F",
                            "pic.twitter.com/Uwqbe2bCVR",
                            "pic.twitter.com/TQ9NHpgpFC",
                            "pic.twitter.com/qik86IMYNb",
                            "pic.twitter.com/MwLSq5dWPr",
                            "pic.twitter.com/ulsysCFNeU"];

                        debug_show_correct = false; // display * after the correct answer
                        if (debug_show_correct)
                            all_questions.forEach(function (item) {
                                item[1] += '*'
                            })

                        // number of questions per run (chosen randomly)
                        var num_active_questions = result_messages.length - 1;
                        //var num_active_questions = 1;
                        var max_options_per_question = 4;  // also selected at random
                        var active_questions, current_question_index, current_score;
                        // questions in current run, initialized at prepare_questions

                        var quiz_div = document.getElementById('semantic_space_surfer');
                        var prompt_text = quiz_div.getElementsByClassName('prompt_text')[0];
                        var answers_box = quiz_div.getElementsByClassName('answer_container')[0];
                        var comment_text = quiz_div.getElementsByClassName('comment_text')[0];
                        var quiz_result = quiz_div.getElementsByClassName('quiz_result')[0];
                        var result_block = quiz_div.getElementsByClassName('result_block')[0];
                        var result_header = quiz_div.getElementsByClassName('result_header')[0];
                        var progressbar = quiz_div.getElementsByClassName('progressbar')[0];
                        var next = quiz_div.getElementsByClassName('next')[0];

                        // list all elements that should be shown and/or hidden in a given situation
                        var hidable_elements = [prompt_text, answers_box, next, comment_text, result_block,
                            quiz_div.getElementsByClassName('hr_in_question')[0]];
                        var elements_for_question = [prompt_text, answers_box, next, comment_text];
                        var elements_for_result = [result_block];

                        function start_quiz() {
                            current_question_index = current_score = 0;
                            active_questions = sample(all_questions, num_active_questions);
                            display_question();
                        }

                        function display_question() {
                            [prompt, correct_option, all_incorrect_options, comments] = active_questions[current_question_index]
                            comments = comments || [];
                            var chosen_options = sample(all_incorrect_options, max_options_per_question - 1);
                            chosen_options.push(correct_option);
                            chosen_options = shuffled(chosen_options);

                            var correct_index = chosen_options.indexOf(correct_option);
                            console.assert(correct_index != -1, "error: correct option was not chosen, this shouldn't happen")

                            prompt_text.innerHTML = prompt;
                            while (answers_box.firstChild)
                                answers_box.removeChild(answers_box.lastChild);
                            options_html = chosen_options.forEach(function (option, chosen_index) {
                                html_raw = `<button class="answer_button"
                                onclick="answer_onclick(${chosen_index}, ${correct_index},
                                                        ${all_incorrect_options.indexOf(option)})">
                              <p class="answer_text_tight">${option}</p></button>`
                                answers_box.appendChild(createElementFromHTML(html_raw))
                            })
                            comment_text.textContent = "Choose your answer"
                            progressbar.value = (current_question_index) / (active_questions.length) * 100;
                            next.onclick = null;

                            hidable_elements.forEach(function (elem) {
                                elem.style.display = "none";
                            });
                            elements_for_question.forEach(function (elem) {
                                elem.style.display = "block";
                            });
                            next.style.opacity = 0.2;
                        }

                        async function answer_onclick(chosen_index, correct_index, comment_index) {
                            var [_, correct_option, incorrect_options, comments] = active_questions[current_question_index]

                            for (let i = 0; i < answers_box.children.length; i++)
                                answers_box.children[i].disabled = true;

                            answers_box.children[chosen_index].style = "background-color: #ebf6fa;"
                            comment_text.textContent = "Drum roll..."
                            await sleep(0);
                            if (chosen_index == correct_index) {
                                current_score++;
                                comment_text.textContent = "Correct";
                                answers_box.children[chosen_index].style = "background-color: #fafff0; box-shadow: 0 6px 12px #6e8f27, 0 4px 4px #6e8f27";
                            } else {
                                comment = comments[comment_index] || '';
                                comment_text.textContent = ((comment != '') ? comment : "Wrong");
                                answers_box.children[chosen_index].style = "background-color: #fcf5f5; box-shadow: 0 6px 12px #b32020, 0 4px 4px #b32020;";

                                answers_box.children[correct_index].style = "background-color: #fafff0; box-shadow: 0 6px 12px #6e8f27, 0 4px 4px #6e8f27;";
                            }
                            next.style.opacity = 1.0;
                            next.onclick = next_onclick;
                        }

                        function next_onclick() {
                            current_question_index++;
                            if (current_question_index < active_questions.length)
                                display_question()
                            else
                                display_result()
                        }

                        function display_result() {

                            console.log(result_certificates[current_score]);
                            reset_html = `<button class="answer_button" onclick="start_quiz()">
                              <p class="answer_text_tight">Try again</p></button>`;
                            twit_text = `I'm%20a%20certified%20Semantic%20Space%20Surfer%20-%20Level%20${current_score}!%20Learn%20about%20Word%20Embeddings%20and%20have%20fun%20at%20%23NLPCourseForYou!%0A${result_certificates[current_score]}%0ATry%20yourself:%20https://lena-voita.github.io/nlp_course/word_embeddings.html%23have_fun`;
                            twit_button = `<a href="https://twitter.com/intent/tweet?text=${twit_text}" target="_blank"><span class="fa fa-twitter" style="margin-right:15px;font-size:30px;"></span>Tweet (with certificate!)</a>`;
                            quiz_result.innerHTML = `${result_messages[current_score]}<br>${reset_html}<br>${twit_button}`;
                            result_header.innerHTML = `Semantic Space Surfer: Level ${current_score}`;
                            progressbar.value = 100;
                            hidable_elements.forEach(function (elem) {
                                elem.style.display = "none";
                            })
                            elements_for_result.forEach(function (elem) {
                                elem.style.display = "block";
                            })

                        }

                        function createElementFromHTML(html) {
                            var div = document.createElement('div');
                            div.innerHTML = html.trim();
                            return div.firstChild;
                        }

                        function sleep(ms) {
                            return new Promise(resolve => setTimeout(resolve, ms));
                        }

                        function sample(array, k) {
                            return array.map(a => [Math.random(), a]).sort().slice(0, k).map(pair => pair[1])
                        }

                        function shuffled(array) {
                            return sample(array, array.length)
                        }

                        // actually draw the first screen
                        start_quiz()

                    </script>

                    <br><br><br><br><br><br>

                </div>
            </div>

        </div>


    </div>


    <footer class="site-footer">
        <div class="wrapper">

            <div class="footer-col-wrapper">
                <div class="footer-col">
                    <p class="text" align="right">&copy; Copyright <font color="#4869df"><strong><a
                                    href="https://lena-voita.github.io/" target="_blank">Lena Voita</a></strong></font>.
                        Translated by <font color="#4869df"><strong>MLNLP</strong></font>. All Rights Reserved
                    </p>
                </div>
            </div>

        </div>
    </footer>


</body>

</html>